{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34135dd4",
   "metadata": {},
   "source": [
    "# 🔧 Data Preprocessing for F1 Probability Modeling\n",
    "## Preparing Singapore GP data for VAE and Bayesian Network implementation\n",
    "\n",
    "This notebook handles:\n",
    "- Feature engineering and selection\n",
    "- Data normalization and scaling\n",
    "- Missing value treatment\n",
    "- Train/validation splits\n",
    "- Feature encoding for ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a2d0b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Preprocessing libraries loaded successfully!\n",
      "⚖️ Feature weighting system enabled: 15 weighted features\n",
      "🎯 Target circuit: Singapore\n",
      "📊 Available circuits: 23\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "import warnings\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# Import configuration and feature weights\n",
    "from config import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"🔧 Preprocessing libraries loaded successfully!\")\n",
    "print(f\"⚖️ Feature weighting system enabled: {len(get_weighted_features('all'))} weighted features\")\n",
    "print(f\"🎯 Target circuit: {DATA_CONFIG['selected_circuit']}\")\n",
    "print(f\"📊 Available circuits: {len(get_available_circuits())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd3d3ab",
   "metadata": {},
   "source": [
    "    # 📂 Load Processed Dataset - Universal Circuit Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cdd4c512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loading weighted prediction dataset...\n",
      "🎯 Loading SINGAPORE GP weighted prediction data from: data/raw\\singapore_prediction_weighted_20251005_165135.csv\n",
      "✅ Data loaded successfully!\n",
      "📊 Shape: (240, 20)\n",
      "🏁 Years: [np.int64(2022), np.int64(2023), np.int64(2024), np.int64(2025)]\n",
      "🏎️ Unique drivers: 30\n",
      "🏁 Unique teams: 13\n",
      "⚖️ Weighted dataset detected!\n",
      "   Weight range: 1.2 - 3.0\n",
      "   Data sources: 12 unique sources\n",
      "   3.0 (critical    ):  60 records\n",
      "   2.5 (high        ): 100 records\n",
      "   1.5 (medium      ):  40 records\n",
      "   1.2 (supplementary):  40 records\n",
      "🔧 Ready for feature engineering...\n"
     ]
    }
   ],
   "source": [
    "# Load the most recent weighted prediction dataset\n",
    "print(\"📂 Loading weighted prediction dataset...\")\n",
    "\n",
    "# Find the most recent weighted prediction data file\n",
    "weighted_files = glob.glob('data/raw/*_prediction_weighted_*.csv')\n",
    "processed_files = glob.glob('data/processed/*_cleaned_*.csv')\n",
    "\n",
    "if weighted_files:\n",
    "    latest_file = max(weighted_files, key=os.path.getctime)\n",
    "    circuit_name = os.path.basename(latest_file).split('_')[0].upper()\n",
    "    print(f\"🎯 Loading {circuit_name} GP weighted prediction data from: {latest_file}\")\n",
    "    df = pd.read_csv(latest_file)\n",
    "    data_source = \"weighted_prediction\"\n",
    "elif processed_files:\n",
    "    latest_file = max(processed_files, key=os.path.getctime)\n",
    "    circuit_name = os.path.basename(latest_file).split('_')[0].upper()\n",
    "    print(f\"📂 Loading processed data from: {latest_file}\")\n",
    "    df = pd.read_csv(latest_file)\n",
    "    data_source = \"processed\"\n",
    "else:\n",
    "    print(\"❌ No weighted prediction data files found. Please run data collection first.\")\n",
    "    df = pd.DataFrame()\n",
    "    data_source = None\n",
    "\n",
    "if not df.empty:\n",
    "    print(f\"✅ Data loaded successfully!\")\n",
    "    print(f\"📊 Shape: {df.shape}\")\n",
    "    print(f\"🏁 Years: {sorted(df['year'].unique())}\")\n",
    "    print(f\"🏎️ Unique drivers: {df['driver_name'].nunique()}\")\n",
    "    print(f\"🏁 Unique teams: {df['team'].nunique()}\")\n",
    "    \n",
    "    # Check for weighted data features\n",
    "    if 'data_weight' in df.columns:\n",
    "        print(f\"⚖️ Weighted dataset detected!\")\n",
    "        print(f\"   Weight range: {df['data_weight'].min():.1f} - {df['data_weight'].max():.1f}\")\n",
    "        print(f\"   Data sources: {df['data_source'].nunique()} unique sources\")\n",
    "        weight_dist = df['data_weight'].value_counts().sort_index(ascending=False)\n",
    "        for weight, count in weight_dist.items():\n",
    "            relevance = df[df['data_weight'] == weight]['prediction_relevance'].iloc[0]\n",
    "            print(f\"   {weight:.1f} ({relevance:12}): {count:3d} records\")\n",
    "    else:\n",
    "        print(\"📊 Standard dataset (no weighting information)\")\n",
    "    \n",
    "    print(f\"🔧 Ready for feature engineering...\")\n",
    "else:\n",
    "    print(\"🚫 Cannot proceed without data. Please run notebook 01 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ebb23f",
   "metadata": {},
   "source": [
    "## 🎯 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff897928",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 📈 Rolling Features & Advanced Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "12edec55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Creating rolling features and advanced engineering...\n",
      "  🏎️ Computing driver form metrics...\n",
      "  🏁 Computing team pace metrics...\n",
      "  ⛽ Computing strategy efficiency...\n",
      "  📊 Computing position movement metrics...\n",
      "  🎯 Computing competitive context...\n",
      "  🏆 Computing experience metrics...\n",
      "✅ Created 13 rolling/advanced features\n",
      "📊 New rolling features: ['abs_pos_change', 'driver_race_form', 'cumulative_races', 'driver_quali_form', 'team_race_avg', 'pit_strategy_delta', 'team_quali_avg', 'quali_vs_teammate', 'driver_points_form', 'pos_efficiency', 'gap_to_pole_normalized', 'years_experience', 'quali_race_delta']\n",
      "\n",
      "🔍 Sample rolling features:\n",
      "    driver_name  year  driver_quali_form  driver_race_form  team_quali_avg  pit_strategy_delta  pos_efficiency\n",
      "Alexander Albon  2022               19.0              17.0       19.500000           -0.583333        0.000000\n",
      "Alexander Albon  2023               16.5              14.0       16.000000            0.416667        0.000000\n",
      "Alexander Albon  2024               12.5              15.5       11.500000           -1.583333        0.000000\n",
      "Alexander Albon  2025               15.0              16.5       11.722222            0.416667        0.000000\n",
      "Alexander Albon  2025               15.5              15.0       11.722222           -1.583333        0.000000\n",
      "Alexander Albon  2025               16.0              16.0       11.722222            0.416667        0.000000\n",
      "Alexander Albon  2025               12.5              10.5       11.722222           -0.583333        4.000000\n",
      "Alexander Albon  2025               10.0               5.5       11.722222            0.416667        0.909091\n",
      "Alexander Albon  2025               12.5               7.0       11.722222            1.416667        1.000000\n",
      "Alexander Albon  2025               14.5              11.0       11.722222            0.416667        0.000000\n",
      "📋 Updated main dataset shape: (240, 33)\n"
     ]
    }
   ],
   "source": [
    "if not df.empty:\n",
    "    print(\"📈 Creating rolling features and advanced engineering...\")\n",
    "    \n",
    "    # Create a copy and sort by driver and year for rolling calculations\n",
    "    df_rolling = df.copy().sort_values(['driver_name', 'year'])\n",
    "    \n",
    "    # 1. Driver Form - Rolling averages (last 2 races per driver)\n",
    "    print(\"  🏎️ Computing driver form metrics...\")\n",
    "    \n",
    "    # Rolling qualifying performance (last 2 races)\n",
    "    df_rolling['driver_quali_form'] = df_rolling.groupby('driver_name')['quali_pos'].rolling(\n",
    "        window=2, min_periods=1\n",
    "    ).mean().reset_index(0, drop=True)\n",
    "    \n",
    "    # Rolling race performance (last 2 races)\n",
    "    df_rolling['driver_race_form'] = df_rolling.groupby('driver_name')['finish_pos'].rolling(\n",
    "        window=2, min_periods=1\n",
    "    ).mean().reset_index(0, drop=True)\n",
    "    \n",
    "    # Rolling points form\n",
    "    df_rolling['driver_points_form'] = df_rolling.groupby('driver_name')['points'].rolling(\n",
    "        window=2, min_periods=1\n",
    "    ).mean().reset_index(0, drop=True)\n",
    "    \n",
    "    # 2. Team Pace Analysis\n",
    "    print(\"  🏁 Computing team pace metrics...\")\n",
    "    \n",
    "    # Team average qualifying position by year\n",
    "    team_quali_avg = df_rolling.groupby(['team', 'year'])['quali_pos'].mean()\n",
    "    df_rolling['team_quali_avg'] = df_rolling.apply(\n",
    "        lambda row: team_quali_avg.get((row['team'], row['year']), row['quali_pos']), axis=1\n",
    "    )\n",
    "    \n",
    "    # Team average race position by year\n",
    "    team_race_avg = df_rolling.groupby(['team', 'year'])['finish_pos'].mean()\n",
    "    df_rolling['team_race_avg'] = df_rolling.apply(\n",
    "        lambda row: team_race_avg.get((row['team'], row['year']), row['finish_pos']), axis=1\n",
    "    )\n",
    "    \n",
    "    # 3. Strategy Efficiency Metrics\n",
    "    print(\"  ⛽ Computing strategy efficiency...\")\n",
    "    \n",
    "    # Pit stop efficiency (compared to team average)\n",
    "    team_pit_avg = df_rolling.groupby('team')['pit_stops'].mean()\n",
    "    df_rolling['pit_strategy_delta'] = df_rolling.apply(\n",
    "        lambda row: row['pit_stops'] - team_pit_avg.get(row['team'], row['pit_stops']), axis=1\n",
    "    )\n",
    "    \n",
    "    # Qualifying vs race performance delta\n",
    "    df_rolling['quali_race_delta'] = df_rolling['finish_pos'] - df_rolling['quali_pos']\n",
    "    \n",
    "    # 4. Position Movement Analytics\n",
    "    print(\"  📊 Computing position movement metrics...\")\n",
    "    \n",
    "    # Absolute position change (regardless of direction)\n",
    "    df_rolling['abs_pos_change'] = abs(df_rolling['pos_change'])\n",
    "    \n",
    "    # Position change efficiency (points gained per position moved)\n",
    "    df_rolling['pos_efficiency'] = np.where(\n",
    "        df_rolling['abs_pos_change'] > 0,\n",
    "        df_rolling['points'] / (df_rolling['abs_pos_change'] + 1),  # +1 to avoid division by zero\n",
    "        df_rolling['points']\n",
    "    )\n",
    "    \n",
    "    # 5. Competitive Context Features\n",
    "    print(\"  🎯 Computing competitive context...\")\n",
    "    \n",
    "    # Gap to pole relative to field (normalized)\n",
    "    year_pole_gaps = df_rolling.groupby('year')['gap_to_pole']\n",
    "    df_rolling['gap_to_pole_normalized'] = (\n",
    "        df_rolling['gap_to_pole'] / year_pole_gaps.transform('max').replace(0, 1)  # Avoid division by zero\n",
    "    ).fillna(0)\n",
    "    \n",
    "    # Driver's qualifying position relative to team mate (if available)\n",
    "    teammate_quali = df_rolling.groupby(['team', 'year'])['quali_pos'].transform('mean')\n",
    "    df_rolling['quali_vs_teammate'] = df_rolling['quali_pos'] - teammate_quali\n",
    "    \n",
    "    # 6. Experience-based Features\n",
    "    print(\"  🏆 Computing experience metrics...\")\n",
    "    \n",
    "    # Cumulative races for each driver (experience progression)\n",
    "    df_rolling['cumulative_races'] = df_rolling.groupby('driver_name').cumcount() + 1\n",
    "    \n",
    "    # Years of experience in dataset\n",
    "    driver_years = df_rolling.groupby('driver_name')['year'].nunique()\n",
    "    df_rolling['years_experience'] = df_rolling['driver_name'].map(driver_years)\n",
    "    \n",
    "    print(f\"✅ Created {len(df_rolling.columns) - len(df.columns)} rolling/advanced features\")\n",
    "    \n",
    "    # Show sample of new features\n",
    "    new_rolling_features = set(df_rolling.columns) - set(df.columns)\n",
    "    print(f\"📊 New rolling features: {list(new_rolling_features)}\")\n",
    "    \n",
    "    # Display sample with key rolling features\n",
    "    sample_cols = [\n",
    "        'driver_name', 'year', 'driver_quali_form', 'driver_race_form', \n",
    "        'team_quali_avg', 'pit_strategy_delta', 'pos_efficiency'\n",
    "    ]\n",
    "    \n",
    "    # Check which columns actually exist\n",
    "    available_sample_cols = [col for col in sample_cols if col in df_rolling.columns]\n",
    "    \n",
    "    if available_sample_cols:\n",
    "        print(f\"\\n🔍 Sample rolling features:\")\n",
    "        print(df_rolling[available_sample_cols].head(10).to_string(index=False))\n",
    "    \n",
    "    # Update main dataframe\n",
    "    df_features = df_rolling.copy()\n",
    "    print(f\"📋 Updated main dataset shape: {df_features.shape}\")\n",
    "else:\n",
    "    print(\"⚠️ No data available for rolling feature engineering.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c44a93df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Creating derived features...\n",
      "  🏆 Computing driver skill ratings...\n",
      "  🏁 Computing team strength ratings...\n",
      "  ⏱️ Computing qualifying performance...\n",
      "  🔧 Computing strategy features...\n",
      "  📚 Computing experience metrics...\n",
      "  📅 Computing temporal features...\n",
      "  🏁 Applying circuit-specific modifiers...\n",
      "✅ Created 24 derived features\n",
      "📊 New derived features: ['best_quali_time', 'pit_strategy_delta', 'strategy_weighted', 'team_quali_avg', 'driver_skill', 'driver_points_form', 'grid_importance_weighted', 'quali_race_delta', 'abs_pos_change', 'driver_experience', 'race_completion_rate', 'team_strength', 'year_normalized', 'gap_to_pole_normalized', 'years_experience', 'is_aggressive_strategy', 'quali_vs_grid', 'driver_race_form', 'cumulative_races', 'pos_efficiency', 'team_race_avg', 'driver_quali_form', 'quali_vs_teammate', 'driver_experience_norm']\n",
      "\n",
      "🔍 Sample derived features:\n",
      "    driver_name  driver_skill  team_strength  driver_experience_norm  is_aggressive_strategy\n",
      "Alexander Albon      0.321833       0.263516                     1.0                       0\n",
      "Alexander Albon      0.321833       0.263516                     1.0                       1\n",
      "Alexander Albon      0.321833       0.263516                     1.0                       0\n",
      "Alexander Albon      0.321833       0.263516                     1.0                       1\n",
      "Alexander Albon      0.321833       0.263516                     1.0                       0\n",
      "🔧 Final engineered dataset shape: (240, 44)\n"
     ]
    }
   ],
   "source": [
    "if not df.empty:\n",
    "    print(\"🎯 Creating derived features...\")\n",
    "    \n",
    "    # Ensure we have df_features from rolling analysis or create from df\n",
    "    if 'df_features' not in locals():\n",
    "        df_features = df.copy()\n",
    "    \n",
    "    # 1. Driver skill rating (based on historical performance)\n",
    "    print(\"  🏆 Computing driver skill ratings...\")\n",
    "    driver_stats = df_features.groupby('driver_name').agg({\n",
    "        'points': 'mean',\n",
    "        'finish_pos': 'mean',\n",
    "        'pos_change': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Normalize driver skill (0-1 scale, higher is better)\n",
    "    driver_stats['skill_score'] = (\n",
    "        (driver_stats['points'] / 25) * 0.4 +  # Points contribution (normalized to max points)\n",
    "        ((21 - driver_stats['finish_pos']) / 20) * 0.4 +  # Avg finish position (inverted, normalized)\n",
    "        ((driver_stats['pos_change'] + 10) / 20) * 0.2  # Position gain ability (normalized)\n",
    "    ).clip(0, 1)\n",
    "    \n",
    "    # Map back to original dataframe\n",
    "    df_features['driver_skill'] = df_features['driver_name'].map(driver_stats['skill_score'])\n",
    "    \n",
    "    # 2. Team performance rating\n",
    "    print(\"  🏁 Computing team strength ratings...\")\n",
    "    team_stats = df_features.groupby('team').agg({\n",
    "        'points': 'mean',\n",
    "        'finish_pos': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Normalize team strength (0-1 scale, higher is better)\n",
    "    max_team_points = team_stats['points'].max() if team_stats['points'].max() > 0 else 1\n",
    "    team_stats['strength_score'] = (\n",
    "        (team_stats['points'] / max_team_points) * 0.6 +  # Points contribution\n",
    "        ((21 - team_stats['finish_pos']) / 20) * 0.4  # Average position (inverted)\n",
    "    ).clip(0, 1)\n",
    "    \n",
    "    df_features['team_strength'] = df_features['team'].map(team_stats['strength_score'])\n",
    "    \n",
    "    # 3. Qualifying performance metrics\n",
    "    print(\"  ⏱️ Computing qualifying performance...\")\n",
    "    \n",
    "    # Qualifying vs grid position difference (penalty/promotion effects)\n",
    "    df_features['quali_vs_grid'] = 0  # Default\n",
    "    valid_quali = (df_features['quali_pos'] > 0) & (df_features['grid_pos'] > 0)\n",
    "    df_features.loc[valid_quali, 'quali_vs_grid'] = (\n",
    "        df_features.loc[valid_quali, 'quali_pos'] - df_features.loc[valid_quali, 'grid_pos']\n",
    "    )\n",
    "    \n",
    "    # Best available qualifying time\n",
    "    def get_best_quali_time(row):\n",
    "        times = [row.get('q3_time', 0), row.get('q2_time', 0), row.get('q1_time', 0)]\n",
    "        valid_times = [t for t in times if t > 0]\n",
    "        return min(valid_times) if valid_times else 0\n",
    "    \n",
    "    df_features['best_quali_time'] = df_features.apply(get_best_quali_time, axis=1)\n",
    "    \n",
    "    # 4. Strategic variables\n",
    "    print(\"  🔧 Computing strategy features...\")\n",
    "    df_features['is_aggressive_strategy'] = (df_features['pit_stops'] >= 2).astype(int)\n",
    "    \n",
    "    # Race completion rate (compared to race winner)\n",
    "    max_laps_by_year = df_features.groupby('year')['total_laps'].max()\n",
    "    df_features['race_completion_rate'] = df_features.apply(\n",
    "        lambda row: (row['total_laps'] / max_laps_by_year.get(row['year'], 1)) if max_laps_by_year.get(row['year'], 0) > 0 else 0,\n",
    "        axis=1\n",
    "    ).clip(0, 1)\n",
    "    \n",
    "    # 5. Experience factor\n",
    "    print(\"  📚 Computing experience metrics...\")\n",
    "    driver_experience = df_features.groupby('driver_name').size()\n",
    "    df_features['driver_experience'] = df_features['driver_name'].map(driver_experience)\n",
    "    \n",
    "    # Normalize experience (0-1 scale)\n",
    "    max_experience = df_features['driver_experience'].max() if df_features['driver_experience'].max() > 0 else 1\n",
    "    df_features['driver_experience_norm'] = (df_features['driver_experience'] / max_experience).clip(0, 1)\n",
    "    \n",
    "    # 6. Temporal normalization\n",
    "    print(\"  📅 Computing temporal features...\")\n",
    "    year_range = df_features['year'].max() - df_features['year'].min()\n",
    "    if year_range > 0:\n",
    "        df_features['year_normalized'] = (df_features['year'] - df_features['year'].min()) / year_range\n",
    "    else:\n",
    "        df_features['year_normalized'] = 0.5  # Single year case\n",
    "    \n",
    "    # 7. Circuit-specific modifiers (using config)\n",
    "    print(\"  🏁 Applying circuit-specific modifiers...\")\n",
    "    circuit_config = get_circuit_prediction_modifiers()\n",
    "    \n",
    "    # Apply grid importance modifier\n",
    "    df_features['grid_importance_weighted'] = (\n",
    "        df_features['grid_pos'] * circuit_config['grid_importance']\n",
    "    )\n",
    "    \n",
    "    # Strategy factor weighting\n",
    "    df_features['strategy_weighted'] = (\n",
    "        df_features['pit_stops'] * circuit_config['strategy_factor']\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Created {len(df_features.columns) - len(df.columns)} derived features\")\n",
    "    \n",
    "    # Display new engineered features\n",
    "    new_features = set(df_features.columns) - set(df.columns) if 'df' in locals() else []\n",
    "    print(f\"📊 New derived features: {list(new_features)}\")\n",
    "    \n",
    "    # Show sample of engineered features\n",
    "    sample_cols = ['driver_name', 'driver_skill', 'team_strength', 'driver_experience_norm', 'is_aggressive_strategy']\n",
    "    available_sample_cols = [col for col in sample_cols if col in df_features.columns]\n",
    "    \n",
    "    if available_sample_cols:\n",
    "        print(f\"\\n🔍 Sample derived features:\")\n",
    "        print(df_features[available_sample_cols].head().to_string(index=False))\n",
    "    \n",
    "    # Fill any remaining NaN values in derived features\n",
    "    derived_feature_cols = [\n",
    "        'driver_skill', 'team_strength', 'driver_experience_norm', \n",
    "        'race_completion_rate', 'year_normalized'\n",
    "    ]\n",
    "    \n",
    "    for col in derived_feature_cols:\n",
    "        if col in df_features.columns:\n",
    "            df_features[col] = df_features[col].fillna(df_features[col].median())\n",
    "    \n",
    "    print(f\"🔧 Final engineered dataset shape: {df_features.shape}\")\n",
    "else:\n",
    "    print(\"⚠️ No data available for feature engineering.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f430f20",
   "metadata": {},
   "source": [
    "## 🧹 Data Cleaning & Missing Value Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c20190d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Handling missing values and data cleaning...\n",
      "📊 Numeric features: 37\n",
      "📊 Categorical features: 4\n",
      "\n",
      "🔍 Missing/Zero value analysis:\n",
      "points               | Missing:  0 | Zeros: 120\n",
      "pos_change           | Missing:  0 | Zeros: 38\n",
      "q2_time              | Missing:  0 | Zeros: 67\n",
      "q3_time              | Missing:  0 | Zeros: 127\n",
      "gap_to_pole          | Missing:  0 | Zeros: 72\n",
      "driver_points_form   | Missing:  0 | Zeros: 80\n",
      "quali_race_delta     | Missing:  0 | Zeros: 37\n",
      "abs_pos_change       | Missing:  0 | Zeros: 38\n",
      "pos_efficiency       | Missing:  0 | Zeros: 120\n",
      "gap_to_pole_normalized | Missing:  0 | Zeros: 72\n",
      "quali_vs_grid        | Missing:  0 | Zeros: 207\n",
      "is_aggressive_strategy | Missing:  0 | Zeros: 47\n",
      "\n",
      "✅ Remaining missing values: 0\n"
     ]
    }
   ],
   "source": [
    "if not df.empty:\n",
    "    print(\"🧹 Handling missing values and data cleaning...\")\n",
    "    \n",
    "    # Identify features to clean\n",
    "    numeric_features = df_features.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_features = ['driver_name', 'team', 'tyres_used', 'status']\n",
    "    \n",
    "    # Remove problematic categorical columns for ML\n",
    "    categorical_features = [col for col in categorical_features if col in df_features.columns]\n",
    "    \n",
    "    print(f\"📊 Numeric features: {len(numeric_features)}\")\n",
    "    print(f\"📊 Categorical features: {len(categorical_features)}\")\n",
    "    \n",
    "    # Handle missing values in numeric features\n",
    "    missing_summary = df_features[numeric_features].isnull().sum()\n",
    "    zero_summary = (df_features[numeric_features] == 0).sum()\n",
    "    \n",
    "    print(\"\\n🔍 Missing/Zero value analysis:\")\n",
    "    for col in numeric_features:\n",
    "        missing_count = missing_summary[col]\n",
    "        zero_count = zero_summary[col]\n",
    "        if missing_count > 0 or zero_count > len(df_features) * 0.1:  # Show if >10% zeros\n",
    "            print(f\"{col:20} | Missing: {missing_count:2d} | Zeros: {zero_count:2d}\")\n",
    "    \n",
    "    # Strategy for different types of missing data\n",
    "    \n",
    "    # 1. Qualifying times: Use median by year\n",
    "    quali_time_cols = ['q1_time', 'q2_time', 'q3_time', 'best_quali_time', 'gap_to_pole']\n",
    "    for col in quali_time_cols:\n",
    "        if col in df_features.columns:\n",
    "            # Replace zeros with NaN first\n",
    "            df_features.loc[df_features[col] == 0, col] = np.nan\n",
    "            # Fill with median by year\n",
    "            df_features[col] = df_features.groupby('year')[col].transform(\n",
    "                lambda x: x.fillna(x.median())\n",
    "            )\n",
    "            # If still NaN, use overall median\n",
    "            df_features[col] = df_features[col].fillna(df_features[col].median())\n",
    "    \n",
    "    # 2. Grid/Finish positions: Use forward fill or median\n",
    "    position_cols = ['grid_pos', 'finish_pos', 'quali_pos']\n",
    "    for col in position_cols:\n",
    "        if col in df_features.columns:\n",
    "            # For positions, 0 usually means missing data\n",
    "            df_features.loc[df_features[col] == 0, col] = np.nan\n",
    "            df_features[col] = df_features[col].fillna(df_features[col].median())\n",
    "    \n",
    "    # 3. Performance metrics: Use median imputation\n",
    "    performance_cols = ['points', 'pit_stops', 'total_laps']\n",
    "    for col in performance_cols:\n",
    "        if col in df_features.columns:\n",
    "            df_features[col] = df_features[col].fillna(df_features[col].median())\n",
    "    \n",
    "    # 4. Engineered features: Fill with defaults\n",
    "    df_features['driver_skill'] = df_features['driver_skill'].fillna(0.5)  # Average skill\n",
    "    df_features['team_strength'] = df_features['team_strength'].fillna(0.5)  # Average team\n",
    "    df_features['race_completion_rate'] = df_features['race_completion_rate'].fillna(0.0)\n",
    "    \n",
    "    # Final check for remaining missing values\n",
    "    remaining_missing = df_features[numeric_features].isnull().sum().sum()\n",
    "    print(f\"\\n✅ Remaining missing values: {remaining_missing}\")\n",
    "    \n",
    "    if remaining_missing > 0:\n",
    "        # Use KNN imputation for remaining missing values\n",
    "        print(\"🔧 Applying KNN imputation for remaining missing values...\")\n",
    "        imputer = KNNImputer(n_neighbors=3)\n",
    "        df_features[numeric_features] = imputer.fit_transform(df_features[numeric_features])\n",
    "        print(\"✅ KNN imputation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "388eb266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Available columns after engineering:\n",
      "Total columns: 44\n",
      "Numeric columns (37): ['year', 'grid_pos', 'finish_pos', 'points', 'pos_change', 'quali_pos', 'q1_time', 'q2_time', 'q3_time', 'total_laps', 'pit_stops', 'gap_to_pole', 'data_weight', 'driver_quali_form', 'driver_race_form', 'driver_points_form', 'team_quali_avg', 'team_race_avg', 'pit_strategy_delta', 'quali_race_delta', 'abs_pos_change', 'pos_efficiency', 'gap_to_pole_normalized', 'quali_vs_teammate', 'cumulative_races', 'years_experience', 'driver_skill', 'team_strength', 'quali_vs_grid', 'best_quali_time', 'is_aggressive_strategy', 'race_completion_rate', 'driver_experience', 'driver_experience_norm', 'year_normalized', 'grid_importance_weighted', 'strategy_weighted']\n",
      "String columns (7): ['driver_name', 'driver_abbr', 'team', 'status', 'tyres_used', 'data_source', 'prediction_relevance']\n",
      "  driver_name: object - Sample: Alexander Albon\n",
      "  driver_abbr: object - Sample: ALB\n",
      "  team: object - Sample: Williams\n",
      "  status: object - Sample: Collision damage\n",
      "  tyres_used: object - Sample: SOFT,MEDIUM\n",
      "  data_source: object - Sample: target_circuit_2022\n",
      "  prediction_relevance: object - Sample: critical\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check our available features\n",
    "print(f\"\\n🔍 Available columns after engineering:\")\n",
    "print(f\"Total columns: {len(df_features.columns)}\")\n",
    "numeric_cols = df_features.select_dtypes(include=[np.number]).columns.tolist()\n",
    "string_cols = df_features.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Numeric columns ({len(numeric_cols)}): {numeric_cols}\")\n",
    "print(f\"String columns ({len(string_cols)}): {string_cols}\")\n",
    "\n",
    "# Show sample of problematic columns\n",
    "for col in string_cols:\n",
    "    print(f\"  {col}: {df_features[col].dtype} - Sample: {df_features[col].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2e9e3f",
   "metadata": {},
   "source": [
    "## 📏 Feature Selection & Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2802a7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📏 Feature selection and scaling with importance weighting...\n",
      "📊 Available numeric features: 34\n",
      "🎯 Target variable: finish_pos\n",
      "\n",
      "⚖️ Feature importance summary:\n",
      "  • High importance: 5 features (weight ≥ 0.8)\n",
      "  • Medium importance: 6 features (weight ≥ 0.6)\n",
      "  • All weighted: 16 features\n",
      "\n",
      "🏆 Top weighted features:\n",
      "  • grid_pos                 : 0.95\n",
      "  • quali_pos                : 0.90\n",
      "  • team_strength            : 0.85\n",
      "  • driver_skill             : 0.80\n",
      "  • gap_to_pole              : 0.75\n",
      "  • gap_to_pole_normalized   : 0.75\n",
      "  • q3_time                  : 0.65\n",
      "  • pit_stops                : 0.60\n",
      "  • q2_time                  : 0.55\n",
      "  • driver_experience        : 0.50\n",
      "\n",
      "📊 Feature set composition:\n",
      "  • Core: 5 features\n",
      "  • VAE optimized: 11 features\n",
      "  • Extended: 16 features\n",
      "\n",
      "🔧 Preparing core_weighted dataset...\n",
      "  📊 Using 5 features: ['grid_pos', 'quali_pos', 'team_strength', 'driver_skill', 'gap_to_pole']\n",
      "  📊 Shape after cleaning: X(240, 5), y(240,)\n",
      "  ⚖️ Applied weights (range: 0.75 - 0.95)\n",
      "  ✅ core_weighted dataset ready\n",
      "\n",
      "🔧 Preparing vae_optimized dataset...\n",
      "  📊 Using 11 features: ['grid_pos', 'quali_pos', 'team_strength', 'driver_skill', 'gap_to_pole']...\n",
      "  📊 Shape after cleaning: X(240, 11), y(240,)\n",
      "  ⚖️ Applied weights (range: 0.45 - 0.95)\n",
      "  ✅ vae_optimized dataset ready\n",
      "\n",
      "🔧 Preparing extended_weighted dataset...\n",
      "  📊 Using 16 features: ['grid_pos', 'quali_pos', 'team_strength', 'driver_skill', 'gap_to_pole']...\n",
      "  📊 Shape after cleaning: X(240, 16), y(240,)\n",
      "  ⚖️ Applied weights (range: 0.15 - 0.95)\n",
      "  ✅ extended_weighted dataset ready\n",
      "\n",
      "📈 Feature weighting verification (core_weighted):\n",
      "  grid_pos                  | Weight: 0.950 | Weighted std:  0.952\n",
      "  quali_pos                 | Weight: 0.900 | Weighted std:  0.902\n",
      "  team_strength             | Weight: 0.850 | Weighted std:  0.852\n",
      "  driver_skill              | Weight: 0.800 | Weighted std:  0.802\n",
      "  gap_to_pole               | Weight: 0.750 | Weighted std:  0.752\n",
      "\n",
      "✅ Feature selection and weighting complete!\n",
      "📦 Created 3 weighted datasets: ['core_weighted', 'vae_optimized', 'extended_weighted']\n"
     ]
    }
   ],
   "source": [
    "if not df.empty:\n",
    "    print(\"📏 Feature selection and scaling with importance weighting...\")\n",
    "    \n",
    "    # Get numeric columns only (exclude string columns)\n",
    "    numeric_columns = df_features.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    target = 'finish_pos'\n",
    "    \n",
    "    # Remove target and any non-predictive columns\n",
    "    exclude_cols = [target, 'year', 'points']  # Points is result, not predictor\n",
    "    available_features = [col for col in numeric_columns if col not in exclude_cols]\n",
    "    \n",
    "    print(f\"📊 Available numeric features: {len(available_features)}\")\n",
    "    print(f\"🎯 Target variable: {target}\")\n",
    "    \n",
    "    # Get weighted features from config\n",
    "    config_weighted = get_weighted_features(\"all\")\n",
    "    config_high = get_weighted_features(\"high\")\n",
    "    config_medium = get_weighted_features(\"medium\")\n",
    "    \n",
    "    # Map config feature names to actual column names\n",
    "    def map_config_to_columns(config_features, available_cols):\n",
    "        \"\"\"Map config feature names to actual DataFrame columns\"\"\"\n",
    "        mapped = {}\n",
    "        for config_name, weight in config_features.items():\n",
    "            # Direct match\n",
    "            if config_name in available_cols:\n",
    "                mapped[config_name] = weight\n",
    "            # Pattern matching for similar names\n",
    "            else:\n",
    "                for col in available_cols:\n",
    "                    if config_name.replace('_', '') in col.replace('_', '') or col.replace('_', '') in config_name.replace('_', ''):\n",
    "                        mapped[col] = weight\n",
    "                        break\n",
    "        return mapped\n",
    "    \n",
    "    # Map config weights to available features\n",
    "    available_weighted = map_config_to_columns(config_weighted, available_features)\n",
    "    available_high = map_config_to_columns(config_high, available_features)\n",
    "    available_medium = map_config_to_columns(config_medium, available_features)\n",
    "    \n",
    "    # Add default weights for important features not in config\n",
    "    important_patterns = {\n",
    "        'driver_skill': 0.80,\n",
    "        'team_strength': 0.85,\n",
    "        'grid_pos': 0.95,\n",
    "        'quali_pos': 0.90,\n",
    "        'pit_stops': 0.60,\n",
    "        'gap_to_pole': 0.75,\n",
    "        'q3_time': 0.65,\n",
    "        'q2_time': 0.55,\n",
    "        'driver_experience': 0.50,\n",
    "        'year_normalized': 0.45\n",
    "    }\n",
    "    \n",
    "    for pattern, weight in important_patterns.items():\n",
    "        for col in available_features:\n",
    "            if pattern in col and col not in available_weighted:\n",
    "                if weight >= 0.8:\n",
    "                    available_high[col] = weight\n",
    "                elif weight >= 0.6:\n",
    "                    available_medium[col] = weight\n",
    "                available_weighted[col] = weight\n",
    "    \n",
    "    print(f\"\\n⚖️ Feature importance summary:\")\n",
    "    print(f\"  • High importance: {len(available_high)} features (weight ≥ 0.8)\")\n",
    "    print(f\"  • Medium importance: {len(available_medium)} features (weight ≥ 0.6)\")\n",
    "    print(f\"  • All weighted: {len(available_weighted)} features\")\n",
    "    \n",
    "    # Show top weighted features\n",
    "    top_weighted = dict(sorted(available_weighted.items(), key=lambda x: x[1], reverse=True)[:10])\n",
    "    print(f\"\\n🏆 Top weighted features:\")\n",
    "    for feature, weight in top_weighted.items():\n",
    "        print(f\"  • {feature:25}: {weight:.2f}\")\n",
    "    \n",
    "    # Create feature sets\n",
    "    core_features = list(available_high.keys())[:8]  # Top 8 high importance\n",
    "    vae_features = list(available_high.keys()) + list(available_medium.keys())\n",
    "    vae_features = list(dict.fromkeys(vae_features))[:15]  # Remove duplicates, limit to 15\n",
    "    extended_features = list(available_weighted.keys())[:20]  # Top 20 weighted features\n",
    "    \n",
    "    print(f\"\\n📊 Feature set composition:\")\n",
    "    print(f\"  • Core: {len(core_features)} features\")\n",
    "    print(f\"  • VAE optimized: {len(vae_features)} features\") \n",
    "    print(f\"  • Extended: {len(extended_features)} features\")\n",
    "    \n",
    "    # Create feature sets with weights\n",
    "    feature_sets = {\n",
    "        'core_weighted': {\n",
    "            'features': core_features,\n",
    "            'weights': {f: available_weighted.get(f, 0.5) for f in core_features}\n",
    "        },\n",
    "        'vae_optimized': {\n",
    "            'features': vae_features,\n",
    "            'weights': {f: available_weighted.get(f, 0.5) for f in vae_features}\n",
    "        },\n",
    "        'extended_weighted': {\n",
    "            'features': extended_features,\n",
    "            'weights': {f: available_weighted.get(f, 0.3) for f in extended_features}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Prepare weighted datasets\n",
    "    datasets = {}\n",
    "    scalers = {}\n",
    "    \n",
    "    for set_name, fset in feature_sets.items():\n",
    "        print(f\"\\n🔧 Preparing {set_name} dataset...\")\n",
    "        \n",
    "        features = fset['features']\n",
    "        weights = fset['weights']\n",
    "        \n",
    "        # Skip if no features available\n",
    "        if len(features) == 0:\n",
    "            print(f\"  ⚠️ No features available for {set_name}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Ensure all features exist in dataframe\n",
    "        existing_features = [f for f in features if f in df_features.columns]\n",
    "        if len(existing_features) != len(features):\n",
    "            missing = set(features) - set(existing_features)\n",
    "            print(f\"  ⚠️ Missing features: {missing}\")\n",
    "            features = existing_features\n",
    "            weights = {f: weights[f] for f in existing_features}\n",
    "        \n",
    "        print(f\"  📊 Using {len(features)} features: {features[:5]}{'...' if len(features) > 5 else ''}\")\n",
    "        \n",
    "        # Extract features and target\n",
    "        X = df_features[features].copy()\n",
    "        y = df_features[target].copy()\n",
    "        \n",
    "        # Remove any rows with missing values\n",
    "        valid_mask = ~(X.isnull().any(axis=1) | y.isnull())\n",
    "        X = X[valid_mask]\n",
    "        y = y[valid_mask]\n",
    "        \n",
    "        print(f\"  📊 Shape after cleaning: X{X.shape}, y{y.shape}\")\n",
    "        \n",
    "        if len(X) == 0:\n",
    "            print(f\"  ❌ No valid data for {set_name}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "        \n",
    "        # Apply feature weights by multiplying scaled features\n",
    "        feature_weights_array = np.array([weights.get(col, 0.5) for col in X_scaled.columns])\n",
    "        X_weighted = X_scaled * feature_weights_array\n",
    "        \n",
    "        print(f\"  ⚖️ Applied weights (range: {feature_weights_array.min():.2f} - {feature_weights_array.max():.2f})\")\n",
    "        \n",
    "        # Store dataset and scaler\n",
    "        datasets[set_name] = {\n",
    "            'X': X_weighted,  # Weighted and scaled\n",
    "            'X_scaled': X_scaled,  # Only scaled\n",
    "            'X_raw': X,  # Raw features\n",
    "            'y': y,\n",
    "            'features': features,\n",
    "            'weights': weights\n",
    "        }\n",
    "        scalers[set_name] = scaler\n",
    "        \n",
    "        print(f\"  ✅ {set_name} dataset ready\")\n",
    "    \n",
    "    # Display feature weighting verification for first available dataset\n",
    "    if datasets:\n",
    "        first_dataset_name = list(datasets.keys())[0]\n",
    "        first_dataset = datasets[first_dataset_name]\n",
    "        \n",
    "        print(f\"\\n📈 Feature weighting verification ({first_dataset_name}):\")\n",
    "        weights = first_dataset['weights']\n",
    "        X = first_dataset['X']\n",
    "        \n",
    "        for col in list(X.columns)[:5]:  # Show first 5 features\n",
    "            weight = weights[col]\n",
    "            weighted_std = X[col].std()\n",
    "            print(f\"  {col:25} | Weight: {weight:.3f} | Weighted std: {weighted_std:6.3f}\")\n",
    "    \n",
    "    print(f\"\\n✅ Feature selection and weighting complete!\")\n",
    "    print(f\"📦 Created {len(datasets)} weighted datasets: {list(datasets.keys())}\")\n",
    "else:\n",
    "    print(\"⚠️ No data available for feature selection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434dda54",
   "metadata": {},
   "source": [
    "## 🎲 Train/Validation Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af97115f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎲 Creating train/validation splits...\n",
      "\n",
      "🔄 Splitting core_weighted dataset...\n",
      "  📅 Temporal split - Train: 60 | Val: 180\n",
      "    Train years: [np.int64(2022), np.int64(2023), np.int64(2024)] | Val years: [np.int64(2025)]\n",
      "  🎯 Stratified split - Train: 180 | Val: 60\n",
      "\n",
      "🔄 Splitting vae_optimized dataset...\n",
      "  📅 Temporal split - Train: 60 | Val: 180\n",
      "    Train years: [np.int64(2022), np.int64(2023), np.int64(2024)] | Val years: [np.int64(2025)]\n",
      "  🎯 Stratified split - Train: 180 | Val: 60\n",
      "\n",
      "🔄 Splitting extended_weighted dataset...\n",
      "  📅 Temporal split - Train: 60 | Val: 180\n",
      "    Train years: [np.int64(2022), np.int64(2023), np.int64(2024)] | Val years: [np.int64(2025)]\n",
      "  🎯 Stratified split - Train: 180 | Val: 60\n",
      "\n",
      "📊 Split analysis for core_weighted dataset:\n",
      "  📅 Temporal: 60 train, 180 val\n",
      "  🎯 Stratified: 180 train, 60 val\n",
      "\n",
      "📈 Target distribution (finish position):\n",
      "  Train: mean=10.5, std=5.7\n",
      "  Val:   mean=10.4, std=5.8\n",
      "\n",
      "✅ All splits created successfully!\n",
      "📦 Split datasets: ['core_weighted', 'vae_optimized', 'extended_weighted']\n"
     ]
    }
   ],
   "source": [
    "if not df.empty:\n",
    "    print(\"🎲 Creating train/validation splits...\")\n",
    "    \n",
    "    splits = {}\n",
    "    \n",
    "    for set_name, dataset in datasets.items():\n",
    "        print(f\"\\n🔄 Splitting {set_name} dataset...\")\n",
    "        \n",
    "        X = dataset['X']\n",
    "        y = dataset['y']\n",
    "        \n",
    "        # Strategy 1: Temporal split (by year)\n",
    "        years = df_features.loc[X.index, 'year']\n",
    "        unique_years = sorted(years.unique())\n",
    "        \n",
    "        if len(unique_years) >= 2:\n",
    "            # Use earliest year(s) for training, latest for validation\n",
    "            if len(unique_years) == 2:\n",
    "                train_years = [unique_years[0]]\n",
    "                val_years = [unique_years[1]]\n",
    "            else:\n",
    "                train_years = unique_years[:-1]\n",
    "                val_years = [unique_years[-1]]\n",
    "            \n",
    "            temporal_train_mask = years.isin(train_years)\n",
    "            temporal_val_mask = years.isin(val_years)\n",
    "            \n",
    "            X_train_temporal = X[temporal_train_mask]\n",
    "            X_val_temporal = X[temporal_val_mask]\n",
    "            y_train_temporal = y[temporal_train_mask]\n",
    "            y_val_temporal = y[temporal_val_mask]\n",
    "            \n",
    "            print(f\"  📅 Temporal split - Train: {X_train_temporal.shape[0]} | Val: {X_val_temporal.shape[0]}\")\n",
    "            print(f\"    Train years: {train_years} | Val years: {val_years}\")\n",
    "        else:\n",
    "            X_train_temporal = X_val_temporal = None\n",
    "            y_train_temporal = y_val_temporal = None\n",
    "            print(f\"  ⚠️ Insufficient years for temporal split\")\n",
    "        \n",
    "        # Strategy 2: Stratified random split (maintaining position distribution)\n",
    "        # Bin finish positions for stratification\n",
    "        y_binned = pd.cut(y, bins=5, labels=['Top5', 'Mid-High', 'Middle', 'Mid-Low', 'Bottom'])\n",
    "        \n",
    "        try:\n",
    "            X_train_strat, X_val_strat, y_train_strat, y_val_strat = train_test_split(\n",
    "                X, y, test_size=0.25, stratify=y_binned, random_state=42\n",
    "            )\n",
    "            print(f\"  🎯 Stratified split - Train: {X_train_strat.shape[0]} | Val: {X_val_strat.shape[0]}\")\n",
    "        except ValueError:\n",
    "            # Fallback to regular random split if stratification fails\n",
    "            X_train_strat, X_val_strat, y_train_strat, y_val_strat = train_test_split(\n",
    "                X, y, test_size=0.25, random_state=42\n",
    "            )\n",
    "            print(f\"  🔀 Random split - Train: {X_train_strat.shape[0]} | Val: {X_val_strat.shape[0]}\")\n",
    "        \n",
    "        # Store splits\n",
    "        splits[set_name] = {\n",
    "            'temporal': {\n",
    "                'X_train': X_train_temporal,\n",
    "                'X_val': X_val_temporal,\n",
    "                'y_train': y_train_temporal,\n",
    "                'y_val': y_val_temporal\n",
    "            },\n",
    "            'stratified': {\n",
    "                'X_train': X_train_strat,\n",
    "                'X_val': X_val_strat,\n",
    "                'y_train': y_train_strat,\n",
    "                'y_val': y_val_strat\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Create simple visualization for core dataset\n",
    "    if datasets:\n",
    "        first_dataset_name = list(datasets.keys())[0]\n",
    "        first_splits = splits[first_dataset_name]\n",
    "        \n",
    "        print(f\"\\n📊 Split analysis for {first_dataset_name} dataset:\")\n",
    "        \n",
    "        # Temporal split stats\n",
    "        if first_splits['temporal']['X_train'] is not None:\n",
    "            temporal_train_size = len(first_splits['temporal']['X_train'])\n",
    "            temporal_val_size = len(first_splits['temporal']['X_val'])\n",
    "            print(f\"  📅 Temporal: {temporal_train_size} train, {temporal_val_size} val\")\n",
    "        \n",
    "        # Stratified split stats  \n",
    "        strat_train_size = len(first_splits['stratified']['X_train'])\n",
    "        strat_val_size = len(first_splits['stratified']['X_val'])\n",
    "        print(f\"  🎯 Stratified: {strat_train_size} train, {strat_val_size} val\")\n",
    "        \n",
    "        # Show target distribution in stratified splits\n",
    "        y_train = first_splits['stratified']['y_train']\n",
    "        y_val = first_splits['stratified']['y_val']\n",
    "        \n",
    "        print(f\"\\n📈 Target distribution (finish position):\")\n",
    "        print(f\"  Train: mean={y_train.mean():.1f}, std={y_train.std():.1f}\")\n",
    "        print(f\"  Val:   mean={y_val.mean():.1f}, std={y_val.std():.1f}\")\n",
    "    \n",
    "    print(\"\\n✅ All splits created successfully!\")\n",
    "    print(f\"📦 Split datasets: {list(splits.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b05ef25",
   "metadata": {},
   "source": [
    "## 📝 Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c943a43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Encoding categorical features...\n",
      "  🏎️ Encoded 30 drivers\n",
      "  🏁 Encoded 13 teams\n",
      "  📅 Encoded 4 years\n",
      "  🎯 Created position bins: ['Podium_Contender', 'Points_Scorer', 'Midfield', 'Backmarker']\n",
      "\n",
      "📊 Categorical distributions:\n",
      "\n",
      "DRIVER (30 categories):\n",
      "Alexander Albon     1\n",
      "Carlos Sainz        1\n",
      "Charles Leclerc     1\n",
      "Daniel Ricciardo    1\n",
      "Esteban Ocon        1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "TEAM (13 categories):\n",
      "Alfa Romeo      1\n",
      "AlphaTauri      1\n",
      "Alpine          1\n",
      "Aston Martin    1\n",
      "Ferrari         1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "YEAR (4 categories):\n",
      "2022    1\n",
      "2023    1\n",
      "2024    1\n",
      "2025    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "FINISH POSITION BINS:\n",
      "finish_pos_binned\n",
      "Points_Scorer       61\n",
      "Podium_Contender    60\n",
      "Midfield            60\n",
      "Backmarker          59\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if not df.empty:\n",
    "    print(\"📝 Encoding categorical features...\")\n",
    "    \n",
    "    # Create categorical encodings for Bayesian Networks\n",
    "    categorical_encodings = {}\n",
    "    \n",
    "    # Driver encoding (for driver effects)\n",
    "    if 'driver_name' in df_features.columns:\n",
    "        driver_encoder = LabelEncoder()\n",
    "        df_features['driver_encoded'] = driver_encoder.fit_transform(df_features['driver_name'])\n",
    "        categorical_encodings['driver'] = {\n",
    "            'encoder': driver_encoder,\n",
    "            'classes': driver_encoder.classes_,\n",
    "            'n_classes': len(driver_encoder.classes_)\n",
    "        }\n",
    "        print(f\"  🏎️ Encoded {len(driver_encoder.classes_)} drivers\")\n",
    "    \n",
    "    # Team encoding\n",
    "    if 'team' in df_features.columns:\n",
    "        team_encoder = LabelEncoder()\n",
    "        df_features['team_encoded'] = team_encoder.fit_transform(df_features['team'])\n",
    "        categorical_encodings['team'] = {\n",
    "            'encoder': team_encoder,\n",
    "            'classes': team_encoder.classes_,\n",
    "            'n_classes': len(team_encoder.classes_)\n",
    "        }\n",
    "        print(f\"  🏁 Encoded {len(team_encoder.classes_)} teams\")\n",
    "    \n",
    "    # Year encoding (for temporal effects)\n",
    "    year_encoder = LabelEncoder()\n",
    "    df_features['year_encoded'] = year_encoder.fit_transform(df_features['year'])\n",
    "    categorical_encodings['year'] = {\n",
    "        'encoder': year_encoder,\n",
    "        'classes': year_encoder.classes_,\n",
    "        'n_classes': len(year_encoder.classes_)\n",
    "    }\n",
    "    print(f\"  📅 Encoded {len(year_encoder.classes_)} years\")\n",
    "    \n",
    "    # Finish position binning (for Bayesian Network discrete variables)\n",
    "    position_bins = [1, 5, 10, 15, 20]  # Top 5, Mid-high (6-10), Mid-low (11-15), Bottom (16-20)\n",
    "    position_labels = ['Podium_Contender', 'Points_Scorer', 'Midfield', 'Backmarker']\n",
    "    \n",
    "    df_features['finish_pos_binned'] = pd.cut(\n",
    "        df_features['finish_pos'], \n",
    "        bins=position_bins, \n",
    "        labels=position_labels, \n",
    "        include_lowest=True\n",
    "    )\n",
    "    \n",
    "    # Encode binned positions\n",
    "    pos_bin_encoder = LabelEncoder()\n",
    "    df_features['finish_pos_binned_encoded'] = pos_bin_encoder.fit_transform(df_features['finish_pos_binned'])\n",
    "    categorical_encodings['finish_pos_binned'] = {\n",
    "        'encoder': pos_bin_encoder,\n",
    "        'classes': pos_bin_encoder.classes_,\n",
    "        'n_classes': len(pos_bin_encoder.classes_),\n",
    "        'bins': position_bins,\n",
    "        'labels': position_labels\n",
    "    }\n",
    "    \n",
    "    print(f\"  🎯 Created position bins: {position_labels}\")\n",
    "    \n",
    "    # Display categorical distribution\n",
    "    print(\"\\n📊 Categorical distributions:\")\n",
    "    for name, encoding in categorical_encodings.items():\n",
    "        if name != 'finish_pos_binned':  # Skip position bins for now\n",
    "            value_counts = pd.Series(encoding['encoder'].inverse_transform(range(encoding['n_classes']))).value_counts()\n",
    "            print(f\"\\n{name.upper()} ({encoding['n_classes']} categories):\")\n",
    "            print(value_counts.head())\n",
    "    \n",
    "    # Position bin distribution\n",
    "    print(\"\\nFINISH POSITION BINS:\")\n",
    "    print(df_features['finish_pos_binned'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f79e2b3",
   "metadata": {},
   "source": [
    "## 💾 Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e1b286d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saving preprocessed data and artifacts...\n",
      "📊 Full engineered dataset: data/preprocessed/singapore_engineered_20251005_180927.csv\n",
      "🔧 Preprocessing artifacts: data/preprocessed/preprocessing_artifacts_singapore_20251005_180927.pkl\n",
      "  💾 core_weighted dataset saved\n",
      "  💾 vae_optimized dataset saved\n",
      "  💾 extended_weighted dataset saved\n",
      "📋 Preprocessing summary: data/preprocessed/preprocessing_summary_singapore_20251005_180927.json\n",
      "\n",
      "🎉 PREPROCESSING COMPLETE!\n",
      "============================================================\n",
      "🏁 Circuit: SINGAPORE GP\n",
      "📊 Original data: (240, 20)\n",
      "🔧 Engineered data: (240, 49)\n",
      "⭐ Features created: 29\n",
      "🎯 Feature sets: 3\n",
      "📂 Categorical encodings: 4\n",
      "\n",
      "📦 Dataset Summary:\n",
      "  • core_weighted  :  5 features, 240 samples\n",
      "  • vae_optimized  : 11 features, 240 samples\n",
      "  • extended_weighted: 16 features, 240 samples\n",
      "\n",
      "✅ Ready for VAE and Bayesian Network implementation!\n",
      "\n",
      "🚀 NEXT STEPS:\n",
      "1. 📖 Load artifacts: pickle.load(preprocessing_artifacts.pkl)\n",
      "2. 🧠 VAE Training: Use 'vae_optimized' scaled datasets\n",
      "3. 🕸️ Bayesian Network: Use categorical encodings + raw features\n",
      "4. 🎲 Validation: Use both temporal and stratified splits\n",
      "5. 🎯 Target: finish_pos (continuous) or finish_pos_binned (discrete)\n",
      "\n",
      "📁 Key Files Created:\n",
      "  🔧 Artifacts: data/preprocessed/preprocessing_artifacts_singapore_20251005_180927.pkl\n",
      "  📊 Engineered Data: data/preprocessed/singapore_engineered_20251005_180927.csv\n",
      "  📋 Summary: data/preprocessed/preprocessing_summary_singapore_20251005_180927.json\n"
     ]
    }
   ],
   "source": [
    "if not df.empty and 'datasets' in locals() and datasets:\n",
    "    print(\"💾 Saving preprocessed data and artifacts...\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs('data/preprocessed', exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Determine circuit name for filename\n",
    "    circuit_name = \"singapore\"  # Default\n",
    "    if data_source == \"weighted_prediction\" and 'data_source' in df.columns:\n",
    "        sample_source = df['data_source'].iloc[0].lower()\n",
    "        if 'singapore' in sample_source:\n",
    "            circuit_name = \"singapore\"\n",
    "        elif 'monaco' in sample_source:\n",
    "            circuit_name = \"monaco\"\n",
    "        elif 'japan' in sample_source:\n",
    "            circuit_name = \"japan\"\n",
    "    \n",
    "    # 1. Save the complete engineered dataset\n",
    "    full_dataset_path = f\"data/preprocessed/{circuit_name}_engineered_{timestamp}.csv\"\n",
    "    df_features.to_csv(full_dataset_path, index=False)\n",
    "    print(f\"📊 Full engineered dataset: {full_dataset_path}\")\n",
    "    \n",
    "    # 2. Save feature sets and splits\n",
    "    preprocessing_artifacts = {\n",
    "        'datasets': datasets,\n",
    "        'splits': splits if 'splits' in locals() else {},\n",
    "        'scalers': scalers,\n",
    "        'categorical_encodings': categorical_encodings if 'categorical_encodings' in locals() else {},\n",
    "        'feature_sets': feature_sets if 'feature_sets' in locals() else {},\n",
    "        'circuit_name': circuit_name,\n",
    "        'timestamp': timestamp,\n",
    "        'original_shape': df.shape,\n",
    "        'engineered_shape': df_features.shape\n",
    "    }\n",
    "    \n",
    "    artifacts_path = f\"data/preprocessed/preprocessing_artifacts_{circuit_name}_{timestamp}.pkl\"\n",
    "    with open(artifacts_path, 'wb') as f:\n",
    "        pickle.dump(preprocessing_artifacts, f)\n",
    "    print(f\"🔧 Preprocessing artifacts: {artifacts_path}\")\n",
    "    \n",
    "    # 3. Save individual datasets for easy loading\n",
    "    for set_name, dataset in datasets.items():\n",
    "        # Scaled features\n",
    "        dataset['X'].to_csv(f\"data/preprocessed/{circuit_name}_{set_name}_X_scaled_{timestamp}.csv\")\n",
    "        # Raw features\n",
    "        dataset['X_raw'].to_csv(f\"data/preprocessed/{circuit_name}_{set_name}_X_raw_{timestamp}.csv\")\n",
    "        # Target\n",
    "        dataset['y'].to_csv(f\"data/preprocessed/{circuit_name}_{set_name}_y_{timestamp}.csv\")\n",
    "        print(f\"  💾 {set_name} dataset saved\")\n",
    "    \n",
    "    # 4. Create preprocessing summary\n",
    "    summary = {\n",
    "        'circuit': circuit_name,\n",
    "        'timestamp': timestamp,\n",
    "        'original_shape': list(df.shape),\n",
    "        'engineered_shape': list(df_features.shape),\n",
    "        'features_created': len(df_features.columns) - len(df.columns),\n",
    "        'datasets_created': len(datasets),\n",
    "        'dataset_info': {\n",
    "            set_name: {\n",
    "                'feature_count': len(dataset['features']),\n",
    "                'sample_count': len(dataset['X']),\n",
    "                'feature_list': dataset['features'][:10]  # First 10 features\n",
    "            } for set_name, dataset in datasets.items()\n",
    "        },\n",
    "        'categorical_encodings': len(categorical_encodings) if 'categorical_encodings' in locals() else 0,\n",
    "        'data_quality': {\n",
    "            'missing_values_handled': True,\n",
    "            'features_scaled': True,\n",
    "            'weights_applied': True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    summary_path = f\"data/preprocessed/preprocessing_summary_{circuit_name}_{timestamp}.json\"\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(f\"📋 Preprocessing summary: {summary_path}\")\n",
    "    \n",
    "    # Display final summary\n",
    "    print(\"\\n🎉 PREPROCESSING COMPLETE!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"🏁 Circuit: {circuit_name.upper()} GP\")\n",
    "    print(f\"📊 Original data: {df.shape}\")\n",
    "    print(f\"🔧 Engineered data: {df_features.shape}\")\n",
    "    print(f\"⭐ Features created: {len(df_features.columns) - len(df.columns)}\")\n",
    "    print(f\"🎯 Feature sets: {len(datasets)}\")\n",
    "    print(f\"📂 Categorical encodings: {len(categorical_encodings) if 'categorical_encodings' in locals() else 0}\")\n",
    "    \n",
    "    # Dataset breakdown\n",
    "    print(f\"\\n📦 Dataset Summary:\")\n",
    "    for set_name, dataset in datasets.items():\n",
    "        print(f\"  • {set_name:15}: {len(dataset['features']):2d} features, {len(dataset['X']):3d} samples\")\n",
    "    \n",
    "    print(f\"\\n✅ Ready for VAE and Bayesian Network implementation!\")\n",
    "    \n",
    "    # Next steps guidance\n",
    "    print(\"\\n🚀 NEXT STEPS:\")\n",
    "    print(\"1. 📖 Load artifacts: pickle.load(preprocessing_artifacts.pkl)\")\n",
    "    print(\"2. 🧠 VAE Training: Use 'vae_optimized' scaled datasets\")\n",
    "    print(\"3. 🕸️ Bayesian Network: Use categorical encodings + raw features\")\n",
    "    print(\"4. 🎲 Validation: Use both temporal and stratified splits\")\n",
    "    print(\"5. 🎯 Target: finish_pos (continuous) or finish_pos_binned (discrete)\")\n",
    "    \n",
    "    # Show file paths for easy reference\n",
    "    print(f\"\\n📁 Key Files Created:\")\n",
    "    print(f\"  🔧 Artifacts: {artifacts_path}\")\n",
    "    print(f\"  📊 Engineered Data: {full_dataset_path}\")\n",
    "    print(f\"  📋 Summary: {summary_path}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Cannot save preprocessing artifacts - no datasets created\")\n",
    "    if df.empty:\n",
    "        print(\"   Reason: No input data loaded\")\n",
    "    elif 'datasets' not in locals():\n",
    "        print(\"   Reason: Feature processing failed\")\n",
    "    elif not datasets:\n",
    "        print(\"   Reason: No valid datasets created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d80c65",
   "metadata": {},
   "source": [
    "## 🔗 VAE → Bayesian Network Integration Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bc847916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Creating VAE → Bayesian Network integration datasets...\n",
      "📊 Using 'vae_optimized' dataset for VAE integration\n",
      "✅ VAE Input Dataset: data/preprocessed/singapore_vae_input_20251005_180927.csv\n",
      "   Shape: (240, 12)\n",
      "   Features: ['grid_pos', 'quali_pos', 'team_strength', 'driver_skill', 'gap_to_pole', 'pit_stops', 'q3_time', 'q2_time', 'driver_experience', 'year_normalized']...\n",
      "✅ BN Input Template: data/preprocessed/singapore_bn_input_template_20251005_180927.csv\n",
      "   Shape: (240, 8)\n",
      "   Features: ['latent_dim_0', 'latent_dim_1', 'latent_dim_2', 'latent_dim_3', 'driver_encoded', 'team_encoded', 'year_encoded', 'finish_pos_binned_encoded']\n",
      "\n",
      "🚀 INTEGRATION WORKFLOW:\n",
      "1. 🧠 Train VAE on vae_input dataset (continuous features)\n",
      "2. 🔄 Encode features → latent vectors (4D)\n",
      "3. 🕸️ Replace latent_dim_* in bn_input_template with VAE output\n",
      "4. 🎯 Train Bayesian Network: latent + categorical → finish_pos_binned\n",
      "5. 🎲 Predict position probabilities using hybrid VAE-BN model\n",
      "\n",
      "📋 Integration summary: data/preprocessed/singapore_vae_bn_integration_20251005_180927.json\n",
      "🎉 VAE → BN integration datasets ready!\n"
     ]
    }
   ],
   "source": [
    "# Create VAE → Bayesian Network integration datasets\n",
    "if not df.empty and 'datasets' in locals() and datasets:\n",
    "    print(\"🔗 Creating VAE → Bayesian Network integration datasets...\")\n",
    "    \n",
    "    # Get the best available dataset for VAE\n",
    "    vae_dataset_name = 'vae_optimized'\n",
    "    if vae_dataset_name not in datasets:\n",
    "        vae_dataset_name = list(datasets.keys())[0]  # Use first available dataset\n",
    "    \n",
    "    print(f\"📊 Using '{vae_dataset_name}' dataset for VAE integration\")\n",
    "    \n",
    "    # VAE Input Dataset (continuous features for encoding)\n",
    "    vae_input_df = datasets[vae_dataset_name]['X_scaled'].copy()  # Use scaled (not weighted) for VAE training\n",
    "    vae_input_df['target'] = datasets[vae_dataset_name]['y']\n",
    "    \n",
    "    # BN Input Dataset Template (will receive VAE latent vectors + categorical features)\n",
    "    if 'categorical_encodings' in locals() and categorical_encodings:\n",
    "        categorical_features = []\n",
    "        for enc_name, enc_info in categorical_encodings.items():\n",
    "            col_name = f\"{enc_name}_encoded\"\n",
    "            if col_name in df_features.columns:\n",
    "                categorical_features.append(col_name)\n",
    "        \n",
    "        if categorical_features:\n",
    "            bn_categorical_df = df_features[categorical_features].copy()\n",
    "        else:\n",
    "            # Create minimal categorical encoding\n",
    "            bn_categorical_df = pd.DataFrame({\n",
    "                'year_encoded': LabelEncoder().fit_transform(df_features['year']),\n",
    "                'driver_encoded': LabelEncoder().fit_transform(df_features['driver_name']),\n",
    "                'team_encoded': LabelEncoder().fit_transform(df_features['team'])\n",
    "            })\n",
    "            categorical_features = list(bn_categorical_df.columns)\n",
    "    else:\n",
    "        # Create minimal categorical encoding\n",
    "        bn_categorical_df = pd.DataFrame({\n",
    "            'year_encoded': LabelEncoder().fit_transform(df_features['year']),\n",
    "            'driver_encoded': LabelEncoder().fit_transform(df_features['driver_name']),\n",
    "            'team_encoded': LabelEncoder().fit_transform(df_features['team'])\n",
    "        })\n",
    "        categorical_features = list(bn_categorical_df.columns)\n",
    "    \n",
    "    # Create BN input structure (latent vector placeholders + categorical)\n",
    "    latent_dims = 4  # VAE latent dimensions\n",
    "    bn_input_df = pd.DataFrame()\n",
    "    \n",
    "    # Add latent dimension placeholders\n",
    "    for i in range(latent_dims):\n",
    "        bn_input_df[f'latent_dim_{i}'] = 0.0  # Will be filled by VAE encoder\n",
    "    \n",
    "    # Add categorical features\n",
    "    for cat_feature in categorical_features:\n",
    "        if cat_feature in bn_categorical_df.columns:\n",
    "            bn_input_df[cat_feature] = bn_categorical_df[cat_feature]\n",
    "    \n",
    "    # Add target (binned for Bayesian Network)\n",
    "    # Create position bins if not exists\n",
    "    if 'finish_pos_binned_encoded' not in df_features.columns:\n",
    "        position_bins = [1, 5, 10, 15, 20]\n",
    "        position_labels = ['Podium_Contender', 'Points_Scorer', 'Midfield', 'Backmarker']\n",
    "        \n",
    "        df_features['finish_pos_binned'] = pd.cut(\n",
    "            df_features['finish_pos'], \n",
    "            bins=position_bins, \n",
    "            labels=position_labels, \n",
    "            include_lowest=True\n",
    "        )\n",
    "        \n",
    "        pos_encoder = LabelEncoder()\n",
    "        df_features['finish_pos_binned_encoded'] = pos_encoder.fit_transform(df_features['finish_pos_binned'])\n",
    "    \n",
    "    bn_input_df['finish_pos_binned_encoded'] = df_features['finish_pos_binned_encoded']\n",
    "    \n",
    "    # Save integration datasets\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    circuit_name = circuit_name if 'circuit_name' in locals() else \"singapore\"\n",
    "    \n",
    "    vae_input_path = f\"data/preprocessed/{circuit_name}_vae_input_{timestamp}.csv\"\n",
    "    bn_input_path = f\"data/preprocessed/{circuit_name}_bn_input_template_{timestamp}.csv\"\n",
    "    \n",
    "    vae_input_df.to_csv(vae_input_path, index=False)\n",
    "    bn_input_df.to_csv(bn_input_path, index=False)\n",
    "    \n",
    "    print(f\"✅ VAE Input Dataset: {vae_input_path}\")\n",
    "    print(f\"   Shape: {vae_input_df.shape}\")\n",
    "    print(f\"   Features: {list(vae_input_df.columns)[:10]}{'...' if len(vae_input_df.columns) > 10 else ''}\")\n",
    "    \n",
    "    print(f\"✅ BN Input Template: {bn_input_path}\")\n",
    "    print(f\"   Shape: {bn_input_df.shape}\")\n",
    "    print(f\"   Features: {list(bn_input_df.columns)}\")\n",
    "    \n",
    "    print(\"\\n🚀 INTEGRATION WORKFLOW:\")\n",
    "    print(\"1. 🧠 Train VAE on vae_input dataset (continuous features)\")\n",
    "    print(\"2. 🔄 Encode features → latent vectors (4D)\")\n",
    "    print(\"3. 🕸️ Replace latent_dim_* in bn_input_template with VAE output\")\n",
    "    print(\"4. 🎯 Train Bayesian Network: latent + categorical → finish_pos_binned\")\n",
    "    print(\"5. 🎲 Predict position probabilities using hybrid VAE-BN model\")\n",
    "    \n",
    "    # Show integration summary\n",
    "    integration_summary = {\n",
    "        'circuit': circuit_name,\n",
    "        'timestamp': timestamp,\n",
    "        'vae_input': {\n",
    "            'path': vae_input_path,\n",
    "            'shape': list(vae_input_df.shape),\n",
    "            'features': list(vae_input_df.columns),\n",
    "            'source_dataset': vae_dataset_name\n",
    "        },\n",
    "        'bn_input_template': {\n",
    "            'path': bn_input_path, \n",
    "            'shape': list(bn_input_df.shape),\n",
    "            'features': list(bn_input_df.columns),\n",
    "            'categorical_features': categorical_features,\n",
    "            'latent_dimensions': latent_dims\n",
    "        },\n",
    "        'integration_workflow': {\n",
    "            'step1': 'Train VAE on continuous features',\n",
    "            'step2': 'Encode features to latent space',\n",
    "            'step3': 'Combine latent + categorical for BN',\n",
    "            'step4': 'Train BN for position prediction',\n",
    "            'final_target': 'finish_pos_binned (4 categories)'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    integration_summary_path = f\"data/preprocessed/{circuit_name}_vae_bn_integration_{timestamp}.json\"\n",
    "    with open(integration_summary_path, 'w') as f:\n",
    "        json.dump(integration_summary, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n📋 Integration summary: {integration_summary_path}\")\n",
    "    print(\"🎉 VAE → BN integration datasets ready!\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ Cannot create VAE-BN integration datasets - no processed datasets available\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
