{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34135dd4",
   "metadata": {},
   "source": [
    "# ğŸ”§ Data Preprocessing for F1 Probability Modeling\n",
    "## Preparing Singapore GP data for VAE and Bayesian Network implementation\n",
    "\n",
    "This notebook handles:\n",
    "- Feature engineering and selection\n",
    "- Data normalization and scaling\n",
    "- Missing value treatment\n",
    "- Train/validation splits\n",
    "- Feature encoding for ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a2d0b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Preprocessing libraries loaded successfully!\n",
      "âš–ï¸ Feature weighting system enabled: 15 weighted features\n",
      "ğŸ¯ Target circuit: Singapore\n",
      "ğŸ“Š Available circuits: 23\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "import warnings\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# Import configuration and feature weights\n",
    "from config import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"ğŸ”§ Preprocessing libraries loaded successfully!\")\n",
    "print(f\"âš–ï¸ Feature weighting system enabled: {len(get_weighted_features('all'))} weighted features\")\n",
    "print(f\"ğŸ¯ Target circuit: {DATA_CONFIG['selected_circuit']}\")\n",
    "print(f\"ğŸ“Š Available circuits: {len(get_available_circuits())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd3d3ab",
   "metadata": {},
   "source": [
    "    # ğŸ“‚ Load Processed Dataset - Universal Circuit Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cdd4c512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Loading weighted prediction dataset...\n",
      "ğŸ¯ Loading SINGAPORE GP weighted prediction data from: data/raw\\singapore_prediction_weighted_20251005_165135.csv\n",
      "âœ… Data loaded successfully!\n",
      "ğŸ“Š Shape: (240, 20)\n",
      "ğŸ Years: [np.int64(2022), np.int64(2023), np.int64(2024), np.int64(2025)]\n",
      "ğŸï¸ Unique drivers: 30\n",
      "ğŸ Unique teams: 13\n",
      "âš–ï¸ Weighted dataset detected!\n",
      "   Weight range: 1.2 - 3.0\n",
      "   Data sources: 12 unique sources\n",
      "   3.0 (critical    ):  60 records\n",
      "   2.5 (high        ): 100 records\n",
      "   1.5 (medium      ):  40 records\n",
      "   1.2 (supplementary):  40 records\n",
      "ğŸ”§ Ready for feature engineering...\n"
     ]
    }
   ],
   "source": [
    "# Load the most recent weighted prediction dataset\n",
    "print(\"ğŸ“‚ Loading weighted prediction dataset...\")\n",
    "\n",
    "# Find the most recent weighted prediction data file\n",
    "weighted_files = glob.glob('data/raw/*_prediction_weighted_*.csv')\n",
    "processed_files = glob.glob('data/processed/*_cleaned_*.csv')\n",
    "\n",
    "if weighted_files:\n",
    "    latest_file = max(weighted_files, key=os.path.getctime)\n",
    "    circuit_name = os.path.basename(latest_file).split('_')[0].upper()\n",
    "    print(f\"ğŸ¯ Loading {circuit_name} GP weighted prediction data from: {latest_file}\")\n",
    "    df = pd.read_csv(latest_file)\n",
    "    data_source = \"weighted_prediction\"\n",
    "elif processed_files:\n",
    "    latest_file = max(processed_files, key=os.path.getctime)\n",
    "    circuit_name = os.path.basename(latest_file).split('_')[0].upper()\n",
    "    print(f\"ğŸ“‚ Loading processed data from: {latest_file}\")\n",
    "    df = pd.read_csv(latest_file)\n",
    "    data_source = \"processed\"\n",
    "else:\n",
    "    print(\"âŒ No weighted prediction data files found. Please run data collection first.\")\n",
    "    df = pd.DataFrame()\n",
    "    data_source = None\n",
    "\n",
    "if not df.empty:\n",
    "    print(f\"âœ… Data loaded successfully!\")\n",
    "    print(f\"ğŸ“Š Shape: {df.shape}\")\n",
    "    print(f\"ğŸ Years: {sorted(df['year'].unique())}\")\n",
    "    print(f\"ğŸï¸ Unique drivers: {df['driver_name'].nunique()}\")\n",
    "    print(f\"ğŸ Unique teams: {df['team'].nunique()}\")\n",
    "    \n",
    "    # Check for weighted data features\n",
    "    if 'data_weight' in df.columns:\n",
    "        print(f\"âš–ï¸ Weighted dataset detected!\")\n",
    "        print(f\"   Weight range: {df['data_weight'].min():.1f} - {df['data_weight'].max():.1f}\")\n",
    "        print(f\"   Data sources: {df['data_source'].nunique()} unique sources\")\n",
    "        weight_dist = df['data_weight'].value_counts().sort_index(ascending=False)\n",
    "        for weight, count in weight_dist.items():\n",
    "            relevance = df[df['data_weight'] == weight]['prediction_relevance'].iloc[0]\n",
    "            print(f\"   {weight:.1f} ({relevance:12}): {count:3d} records\")\n",
    "    else:\n",
    "        print(\"ğŸ“Š Standard dataset (no weighting information)\")\n",
    "    \n",
    "    print(f\"ğŸ”§ Ready for feature engineering...\")\n",
    "else:\n",
    "    print(\"ğŸš« Cannot proceed without data. Please run notebook 01 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ebb23f",
   "metadata": {},
   "source": [
    "## ğŸ¯ Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff897928",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ğŸ“ˆ Rolling Features & Advanced Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "12edec55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ˆ Creating rolling features and advanced engineering...\n",
      "  ğŸï¸ Computing driver form metrics...\n",
      "  ğŸ Computing team pace metrics...\n",
      "  â›½ Computing strategy efficiency...\n",
      "  ğŸ“Š Computing position movement metrics...\n",
      "  ğŸ¯ Computing competitive context...\n",
      "  ğŸ† Computing experience metrics...\n",
      "âœ… Created 13 rolling/advanced features\n",
      "ğŸ“Š New rolling features: ['abs_pos_change', 'driver_race_form', 'cumulative_races', 'driver_quali_form', 'team_race_avg', 'pit_strategy_delta', 'team_quali_avg', 'quali_vs_teammate', 'driver_points_form', 'pos_efficiency', 'gap_to_pole_normalized', 'years_experience', 'quali_race_delta']\n",
      "\n",
      "ğŸ” Sample rolling features:\n",
      "    driver_name  year  driver_quali_form  driver_race_form  team_quali_avg  pit_strategy_delta  pos_efficiency\n",
      "Alexander Albon  2022               19.0              17.0       19.500000           -0.583333        0.000000\n",
      "Alexander Albon  2023               16.5              14.0       16.000000            0.416667        0.000000\n",
      "Alexander Albon  2024               12.5              15.5       11.500000           -1.583333        0.000000\n",
      "Alexander Albon  2025               15.0              16.5       11.722222            0.416667        0.000000\n",
      "Alexander Albon  2025               15.5              15.0       11.722222           -1.583333        0.000000\n",
      "Alexander Albon  2025               16.0              16.0       11.722222            0.416667        0.000000\n",
      "Alexander Albon  2025               12.5              10.5       11.722222           -0.583333        4.000000\n",
      "Alexander Albon  2025               10.0               5.5       11.722222            0.416667        0.909091\n",
      "Alexander Albon  2025               12.5               7.0       11.722222            1.416667        1.000000\n",
      "Alexander Albon  2025               14.5              11.0       11.722222            0.416667        0.000000\n",
      "ğŸ“‹ Updated main dataset shape: (240, 33)\n"
     ]
    }
   ],
   "source": [
    "if not df.empty:\n",
    "    print(\"ğŸ“ˆ Creating rolling features and advanced engineering...\")\n",
    "    \n",
    "    # Create a copy and sort by driver and year for rolling calculations\n",
    "    df_rolling = df.copy().sort_values(['driver_name', 'year'])\n",
    "    \n",
    "    # 1. Driver Form - Rolling averages (last 2 races per driver)\n",
    "    print(\"  ğŸï¸ Computing driver form metrics...\")\n",
    "    \n",
    "    # Rolling qualifying performance (last 2 races)\n",
    "    df_rolling['driver_quali_form'] = df_rolling.groupby('driver_name')['quali_pos'].rolling(\n",
    "        window=2, min_periods=1\n",
    "    ).mean().reset_index(0, drop=True)\n",
    "    \n",
    "    # Rolling race performance (last 2 races)\n",
    "    df_rolling['driver_race_form'] = df_rolling.groupby('driver_name')['finish_pos'].rolling(\n",
    "        window=2, min_periods=1\n",
    "    ).mean().reset_index(0, drop=True)\n",
    "    \n",
    "    # Rolling points form\n",
    "    df_rolling['driver_points_form'] = df_rolling.groupby('driver_name')['points'].rolling(\n",
    "        window=2, min_periods=1\n",
    "    ).mean().reset_index(0, drop=True)\n",
    "    \n",
    "    # 2. Team Pace Analysis\n",
    "    print(\"  ğŸ Computing team pace metrics...\")\n",
    "    \n",
    "    # Team average qualifying position by year\n",
    "    team_quali_avg = df_rolling.groupby(['team', 'year'])['quali_pos'].mean()\n",
    "    df_rolling['team_quali_avg'] = df_rolling.apply(\n",
    "        lambda row: team_quali_avg.get((row['team'], row['year']), row['quali_pos']), axis=1\n",
    "    )\n",
    "    \n",
    "    # Team average race position by year\n",
    "    team_race_avg = df_rolling.groupby(['team', 'year'])['finish_pos'].mean()\n",
    "    df_rolling['team_race_avg'] = df_rolling.apply(\n",
    "        lambda row: team_race_avg.get((row['team'], row['year']), row['finish_pos']), axis=1\n",
    "    )\n",
    "    \n",
    "    # 3. Strategy Efficiency Metrics\n",
    "    print(\"  â›½ Computing strategy efficiency...\")\n",
    "    \n",
    "    # Pit stop efficiency (compared to team average)\n",
    "    team_pit_avg = df_rolling.groupby('team')['pit_stops'].mean()\n",
    "    df_rolling['pit_strategy_delta'] = df_rolling.apply(\n",
    "        lambda row: row['pit_stops'] - team_pit_avg.get(row['team'], row['pit_stops']), axis=1\n",
    "    )\n",
    "    \n",
    "    # Qualifying vs race performance delta\n",
    "    df_rolling['quali_race_delta'] = df_rolling['finish_pos'] - df_rolling['quali_pos']\n",
    "    \n",
    "    # 4. Position Movement Analytics\n",
    "    print(\"  ğŸ“Š Computing position movement metrics...\")\n",
    "    \n",
    "    # Absolute position change (regardless of direction)\n",
    "    df_rolling['abs_pos_change'] = abs(df_rolling['pos_change'])\n",
    "    \n",
    "    # Position change efficiency (points gained per position moved)\n",
    "    df_rolling['pos_efficiency'] = np.where(\n",
    "        df_rolling['abs_pos_change'] > 0,\n",
    "        df_rolling['points'] / (df_rolling['abs_pos_change'] + 1),  # +1 to avoid division by zero\n",
    "        df_rolling['points']\n",
    "    )\n",
    "    \n",
    "    # 5. Competitive Context Features\n",
    "    print(\"  ğŸ¯ Computing competitive context...\")\n",
    "    \n",
    "    # Gap to pole relative to field (normalized)\n",
    "    year_pole_gaps = df_rolling.groupby('year')['gap_to_pole']\n",
    "    df_rolling['gap_to_pole_normalized'] = (\n",
    "        df_rolling['gap_to_pole'] / year_pole_gaps.transform('max').replace(0, 1)  # Avoid division by zero\n",
    "    ).fillna(0)\n",
    "    \n",
    "    # Driver's qualifying position relative to team mate (if available)\n",
    "    teammate_quali = df_rolling.groupby(['team', 'year'])['quali_pos'].transform('mean')\n",
    "    df_rolling['quali_vs_teammate'] = df_rolling['quali_pos'] - teammate_quali\n",
    "    \n",
    "    # 6. Experience-based Features\n",
    "    print(\"  ğŸ† Computing experience metrics...\")\n",
    "    \n",
    "    # Cumulative races for each driver (experience progression)\n",
    "    df_rolling['cumulative_races'] = df_rolling.groupby('driver_name').cumcount() + 1\n",
    "    \n",
    "    # Years of experience in dataset\n",
    "    driver_years = df_rolling.groupby('driver_name')['year'].nunique()\n",
    "    df_rolling['years_experience'] = df_rolling['driver_name'].map(driver_years)\n",
    "    \n",
    "    print(f\"âœ… Created {len(df_rolling.columns) - len(df.columns)} rolling/advanced features\")\n",
    "    \n",
    "    # Show sample of new features\n",
    "    new_rolling_features = set(df_rolling.columns) - set(df.columns)\n",
    "    print(f\"ğŸ“Š New rolling features: {list(new_rolling_features)}\")\n",
    "    \n",
    "    # Display sample with key rolling features\n",
    "    sample_cols = [\n",
    "        'driver_name', 'year', 'driver_quali_form', 'driver_race_form', \n",
    "        'team_quali_avg', 'pit_strategy_delta', 'pos_efficiency'\n",
    "    ]\n",
    "    \n",
    "    # Check which columns actually exist\n",
    "    available_sample_cols = [col for col in sample_cols if col in df_rolling.columns]\n",
    "    \n",
    "    if available_sample_cols:\n",
    "        print(f\"\\nğŸ” Sample rolling features:\")\n",
    "        print(df_rolling[available_sample_cols].head(10).to_string(index=False))\n",
    "    \n",
    "    # Update main dataframe\n",
    "    df_features = df_rolling.copy()\n",
    "    print(f\"ğŸ“‹ Updated main dataset shape: {df_features.shape}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No data available for rolling feature engineering.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c44a93df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Creating derived features...\n",
      "  ğŸ† Computing driver skill ratings...\n",
      "  ğŸ Computing team strength ratings...\n",
      "  â±ï¸ Computing qualifying performance...\n",
      "  ğŸ”§ Computing strategy features...\n",
      "  ğŸ“š Computing experience metrics...\n",
      "  ğŸ“… Computing temporal features...\n",
      "  ğŸ Applying circuit-specific modifiers...\n",
      "âœ… Created 24 derived features\n",
      "ğŸ“Š New derived features: ['best_quali_time', 'pit_strategy_delta', 'strategy_weighted', 'team_quali_avg', 'driver_skill', 'driver_points_form', 'grid_importance_weighted', 'quali_race_delta', 'abs_pos_change', 'driver_experience', 'race_completion_rate', 'team_strength', 'year_normalized', 'gap_to_pole_normalized', 'years_experience', 'is_aggressive_strategy', 'quali_vs_grid', 'driver_race_form', 'cumulative_races', 'pos_efficiency', 'team_race_avg', 'driver_quali_form', 'quali_vs_teammate', 'driver_experience_norm']\n",
      "\n",
      "ğŸ” Sample derived features:\n",
      "    driver_name  driver_skill  team_strength  driver_experience_norm  is_aggressive_strategy\n",
      "Alexander Albon      0.321833       0.263516                     1.0                       0\n",
      "Alexander Albon      0.321833       0.263516                     1.0                       1\n",
      "Alexander Albon      0.321833       0.263516                     1.0                       0\n",
      "Alexander Albon      0.321833       0.263516                     1.0                       1\n",
      "Alexander Albon      0.321833       0.263516                     1.0                       0\n",
      "ğŸ”§ Final engineered dataset shape: (240, 44)\n"
     ]
    }
   ],
   "source": [
    "if not df.empty:\n",
    "    print(\"ğŸ¯ Creating derived features...\")\n",
    "    \n",
    "    # Ensure we have df_features from rolling analysis or create from df\n",
    "    if 'df_features' not in locals():\n",
    "        df_features = df.copy()\n",
    "    \n",
    "    # 1. Driver skill rating (based on historical performance)\n",
    "    print(\"  ğŸ† Computing driver skill ratings...\")\n",
    "    driver_stats = df_features.groupby('driver_name').agg({\n",
    "        'points': 'mean',\n",
    "        'finish_pos': 'mean',\n",
    "        'pos_change': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Normalize driver skill (0-1 scale, higher is better)\n",
    "    driver_stats['skill_score'] = (\n",
    "        (driver_stats['points'] / 25) * 0.4 +  # Points contribution (normalized to max points)\n",
    "        ((21 - driver_stats['finish_pos']) / 20) * 0.4 +  # Avg finish position (inverted, normalized)\n",
    "        ((driver_stats['pos_change'] + 10) / 20) * 0.2  # Position gain ability (normalized)\n",
    "    ).clip(0, 1)\n",
    "    \n",
    "    # Map back to original dataframe\n",
    "    df_features['driver_skill'] = df_features['driver_name'].map(driver_stats['skill_score'])\n",
    "    \n",
    "    # 2. Team performance rating\n",
    "    print(\"  ğŸ Computing team strength ratings...\")\n",
    "    team_stats = df_features.groupby('team').agg({\n",
    "        'points': 'mean',\n",
    "        'finish_pos': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Normalize team strength (0-1 scale, higher is better)\n",
    "    max_team_points = team_stats['points'].max() if team_stats['points'].max() > 0 else 1\n",
    "    team_stats['strength_score'] = (\n",
    "        (team_stats['points'] / max_team_points) * 0.6 +  # Points contribution\n",
    "        ((21 - team_stats['finish_pos']) / 20) * 0.4  # Average position (inverted)\n",
    "    ).clip(0, 1)\n",
    "    \n",
    "    df_features['team_strength'] = df_features['team'].map(team_stats['strength_score'])\n",
    "    \n",
    "    # 3. Qualifying performance metrics\n",
    "    print(\"  â±ï¸ Computing qualifying performance...\")\n",
    "    \n",
    "    # Qualifying vs grid position difference (penalty/promotion effects)\n",
    "    df_features['quali_vs_grid'] = 0  # Default\n",
    "    valid_quali = (df_features['quali_pos'] > 0) & (df_features['grid_pos'] > 0)\n",
    "    df_features.loc[valid_quali, 'quali_vs_grid'] = (\n",
    "        df_features.loc[valid_quali, 'quali_pos'] - df_features.loc[valid_quali, 'grid_pos']\n",
    "    )\n",
    "    \n",
    "    # Best available qualifying time\n",
    "    def get_best_quali_time(row):\n",
    "        times = [row.get('q3_time', 0), row.get('q2_time', 0), row.get('q1_time', 0)]\n",
    "        valid_times = [t for t in times if t > 0]\n",
    "        return min(valid_times) if valid_times else 0\n",
    "    \n",
    "    df_features['best_quali_time'] = df_features.apply(get_best_quali_time, axis=1)\n",
    "    \n",
    "    # 4. Strategic variables\n",
    "    print(\"  ğŸ”§ Computing strategy features...\")\n",
    "    df_features['is_aggressive_strategy'] = (df_features['pit_stops'] >= 2).astype(int)\n",
    "    \n",
    "    # Race completion rate (compared to race winner)\n",
    "    max_laps_by_year = df_features.groupby('year')['total_laps'].max()\n",
    "    df_features['race_completion_rate'] = df_features.apply(\n",
    "        lambda row: (row['total_laps'] / max_laps_by_year.get(row['year'], 1)) if max_laps_by_year.get(row['year'], 0) > 0 else 0,\n",
    "        axis=1\n",
    "    ).clip(0, 1)\n",
    "    \n",
    "    # 5. Experience factor\n",
    "    print(\"  ğŸ“š Computing experience metrics...\")\n",
    "    driver_experience = df_features.groupby('driver_name').size()\n",
    "    df_features['driver_experience'] = df_features['driver_name'].map(driver_experience)\n",
    "    \n",
    "    # Normalize experience (0-1 scale)\n",
    "    max_experience = df_features['driver_experience'].max() if df_features['driver_experience'].max() > 0 else 1\n",
    "    df_features['driver_experience_norm'] = (df_features['driver_experience'] / max_experience).clip(0, 1)\n",
    "    \n",
    "    # 6. Temporal normalization\n",
    "    print(\"  ğŸ“… Computing temporal features...\")\n",
    "    year_range = df_features['year'].max() - df_features['year'].min()\n",
    "    if year_range > 0:\n",
    "        df_features['year_normalized'] = (df_features['year'] - df_features['year'].min()) / year_range\n",
    "    else:\n",
    "        df_features['year_normalized'] = 0.5  # Single year case\n",
    "    \n",
    "    # 7. Circuit-specific modifiers (using config)\n",
    "    print(\"  ğŸ Applying circuit-specific modifiers...\")\n",
    "    circuit_config = get_circuit_prediction_modifiers()\n",
    "    \n",
    "    # Apply grid importance modifier\n",
    "    df_features['grid_importance_weighted'] = (\n",
    "        df_features['grid_pos'] * circuit_config['grid_importance']\n",
    "    )\n",
    "    \n",
    "    # Strategy factor weighting\n",
    "    df_features['strategy_weighted'] = (\n",
    "        df_features['pit_stops'] * circuit_config['strategy_factor']\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Created {len(df_features.columns) - len(df.columns)} derived features\")\n",
    "    \n",
    "    # Display new engineered features\n",
    "    new_features = set(df_features.columns) - set(df.columns) if 'df' in locals() else []\n",
    "    print(f\"ğŸ“Š New derived features: {list(new_features)}\")\n",
    "    \n",
    "    # Show sample of engineered features\n",
    "    sample_cols = ['driver_name', 'driver_skill', 'team_strength', 'driver_experience_norm', 'is_aggressive_strategy']\n",
    "    available_sample_cols = [col for col in sample_cols if col in df_features.columns]\n",
    "    \n",
    "    if available_sample_cols:\n",
    "        print(f\"\\nğŸ” Sample derived features:\")\n",
    "        print(df_features[available_sample_cols].head().to_string(index=False))\n",
    "    \n",
    "    # Fill any remaining NaN values in derived features\n",
    "    derived_feature_cols = [\n",
    "        'driver_skill', 'team_strength', 'driver_experience_norm', \n",
    "        'race_completion_rate', 'year_normalized'\n",
    "    ]\n",
    "    \n",
    "    for col in derived_feature_cols:\n",
    "        if col in df_features.columns:\n",
    "            df_features[col] = df_features[col].fillna(df_features[col].median())\n",
    "    \n",
    "    print(f\"ğŸ”§ Final engineered dataset shape: {df_features.shape}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No data available for feature engineering.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f430f20",
   "metadata": {},
   "source": [
    "## ğŸ§¹ Data Cleaning & Missing Value Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c20190d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹ Handling missing values and data cleaning...\n",
      "ğŸ“Š Numeric features: 37\n",
      "ğŸ“Š Categorical features: 4\n",
      "\n",
      "ğŸ” Missing/Zero value analysis:\n",
      "points               | Missing:  0 | Zeros: 120\n",
      "pos_change           | Missing:  0 | Zeros: 38\n",
      "q2_time              | Missing:  0 | Zeros: 67\n",
      "q3_time              | Missing:  0 | Zeros: 127\n",
      "gap_to_pole          | Missing:  0 | Zeros: 72\n",
      "driver_points_form   | Missing:  0 | Zeros: 80\n",
      "quali_race_delta     | Missing:  0 | Zeros: 37\n",
      "abs_pos_change       | Missing:  0 | Zeros: 38\n",
      "pos_efficiency       | Missing:  0 | Zeros: 120\n",
      "gap_to_pole_normalized | Missing:  0 | Zeros: 72\n",
      "quali_vs_grid        | Missing:  0 | Zeros: 207\n",
      "is_aggressive_strategy | Missing:  0 | Zeros: 47\n",
      "\n",
      "âœ… Remaining missing values: 0\n"
     ]
    }
   ],
   "source": [
    "if not df.empty:\n",
    "    print(\"ğŸ§¹ Handling missing values and data cleaning...\")\n",
    "    \n",
    "    # Identify features to clean\n",
    "    numeric_features = df_features.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_features = ['driver_name', 'team', 'tyres_used', 'status']\n",
    "    \n",
    "    # Remove problematic categorical columns for ML\n",
    "    categorical_features = [col for col in categorical_features if col in df_features.columns]\n",
    "    \n",
    "    print(f\"ğŸ“Š Numeric features: {len(numeric_features)}\")\n",
    "    print(f\"ğŸ“Š Categorical features: {len(categorical_features)}\")\n",
    "    \n",
    "    # Handle missing values in numeric features\n",
    "    missing_summary = df_features[numeric_features].isnull().sum()\n",
    "    zero_summary = (df_features[numeric_features] == 0).sum()\n",
    "    \n",
    "    print(\"\\nğŸ” Missing/Zero value analysis:\")\n",
    "    for col in numeric_features:\n",
    "        missing_count = missing_summary[col]\n",
    "        zero_count = zero_summary[col]\n",
    "        if missing_count > 0 or zero_count > len(df_features) * 0.1:  # Show if >10% zeros\n",
    "            print(f\"{col:20} | Missing: {missing_count:2d} | Zeros: {zero_count:2d}\")\n",
    "    \n",
    "    # Strategy for different types of missing data\n",
    "    \n",
    "    # 1. Qualifying times: Use median by year\n",
    "    quali_time_cols = ['q1_time', 'q2_time', 'q3_time', 'best_quali_time', 'gap_to_pole']\n",
    "    for col in quali_time_cols:\n",
    "        if col in df_features.columns:\n",
    "            # Replace zeros with NaN first\n",
    "            df_features.loc[df_features[col] == 0, col] = np.nan\n",
    "            # Fill with median by year\n",
    "            df_features[col] = df_features.groupby('year')[col].transform(\n",
    "                lambda x: x.fillna(x.median())\n",
    "            )\n",
    "            # If still NaN, use overall median\n",
    "            df_features[col] = df_features[col].fillna(df_features[col].median())\n",
    "    \n",
    "    # 2. Grid/Finish positions: Use forward fill or median\n",
    "    position_cols = ['grid_pos', 'finish_pos', 'quali_pos']\n",
    "    for col in position_cols:\n",
    "        if col in df_features.columns:\n",
    "            # For positions, 0 usually means missing data\n",
    "            df_features.loc[df_features[col] == 0, col] = np.nan\n",
    "            df_features[col] = df_features[col].fillna(df_features[col].median())\n",
    "    \n",
    "    # 3. Performance metrics: Use median imputation\n",
    "    performance_cols = ['points', 'pit_stops', 'total_laps']\n",
    "    for col in performance_cols:\n",
    "        if col in df_features.columns:\n",
    "            df_features[col] = df_features[col].fillna(df_features[col].median())\n",
    "    \n",
    "    # 4. Engineered features: Fill with defaults\n",
    "    df_features['driver_skill'] = df_features['driver_skill'].fillna(0.5)  # Average skill\n",
    "    df_features['team_strength'] = df_features['team_strength'].fillna(0.5)  # Average team\n",
    "    df_features['race_completion_rate'] = df_features['race_completion_rate'].fillna(0.0)\n",
    "    \n",
    "    # Final check for remaining missing values\n",
    "    remaining_missing = df_features[numeric_features].isnull().sum().sum()\n",
    "    print(f\"\\nâœ… Remaining missing values: {remaining_missing}\")\n",
    "    \n",
    "    if remaining_missing > 0:\n",
    "        # Use KNN imputation for remaining missing values\n",
    "        print(\"ğŸ”§ Applying KNN imputation for remaining missing values...\")\n",
    "        imputer = KNNImputer(n_neighbors=3)\n",
    "        df_features[numeric_features] = imputer.fit_transform(df_features[numeric_features])\n",
    "        print(\"âœ… KNN imputation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "388eb266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Available columns after engineering:\n",
      "Total columns: 44\n",
      "Numeric columns (37): ['year', 'grid_pos', 'finish_pos', 'points', 'pos_change', 'quali_pos', 'q1_time', 'q2_time', 'q3_time', 'total_laps', 'pit_stops', 'gap_to_pole', 'data_weight', 'driver_quali_form', 'driver_race_form', 'driver_points_form', 'team_quali_avg', 'team_race_avg', 'pit_strategy_delta', 'quali_race_delta', 'abs_pos_change', 'pos_efficiency', 'gap_to_pole_normalized', 'quali_vs_teammate', 'cumulative_races', 'years_experience', 'driver_skill', 'team_strength', 'quali_vs_grid', 'best_quali_time', 'is_aggressive_strategy', 'race_completion_rate', 'driver_experience', 'driver_experience_norm', 'year_normalized', 'grid_importance_weighted', 'strategy_weighted']\n",
      "String columns (7): ['driver_name', 'driver_abbr', 'team', 'status', 'tyres_used', 'data_source', 'prediction_relevance']\n",
      "  driver_name: object - Sample: Alexander Albon\n",
      "  driver_abbr: object - Sample: ALB\n",
      "  team: object - Sample: Williams\n",
      "  status: object - Sample: Collision damage\n",
      "  tyres_used: object - Sample: SOFT,MEDIUM\n",
      "  data_source: object - Sample: target_circuit_2022\n",
      "  prediction_relevance: object - Sample: critical\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check our available features\n",
    "print(f\"\\nğŸ” Available columns after engineering:\")\n",
    "print(f\"Total columns: {len(df_features.columns)}\")\n",
    "numeric_cols = df_features.select_dtypes(include=[np.number]).columns.tolist()\n",
    "string_cols = df_features.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Numeric columns ({len(numeric_cols)}): {numeric_cols}\")\n",
    "print(f\"String columns ({len(string_cols)}): {string_cols}\")\n",
    "\n",
    "# Show sample of problematic columns\n",
    "for col in string_cols:\n",
    "    print(f\"  {col}: {df_features[col].dtype} - Sample: {df_features[col].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2e9e3f",
   "metadata": {},
   "source": [
    "## ğŸ“ Feature Selection & Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2802a7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Feature selection and scaling with importance weighting...\n",
      "ğŸ“Š Available numeric features: 34\n",
      "ğŸ¯ Target variable: finish_pos\n",
      "\n",
      "âš–ï¸ Feature importance summary:\n",
      "  â€¢ High importance: 5 features (weight â‰¥ 0.8)\n",
      "  â€¢ Medium importance: 6 features (weight â‰¥ 0.6)\n",
      "  â€¢ All weighted: 16 features\n",
      "\n",
      "ğŸ† Top weighted features:\n",
      "  â€¢ grid_pos                 : 0.95\n",
      "  â€¢ quali_pos                : 0.90\n",
      "  â€¢ team_strength            : 0.85\n",
      "  â€¢ driver_skill             : 0.80\n",
      "  â€¢ gap_to_pole              : 0.75\n",
      "  â€¢ gap_to_pole_normalized   : 0.75\n",
      "  â€¢ q3_time                  : 0.65\n",
      "  â€¢ pit_stops                : 0.60\n",
      "  â€¢ q2_time                  : 0.55\n",
      "  â€¢ driver_experience        : 0.50\n",
      "\n",
      "ğŸ“Š Feature set composition:\n",
      "  â€¢ Core: 5 features\n",
      "  â€¢ VAE optimized: 11 features\n",
      "  â€¢ Extended: 16 features\n",
      "\n",
      "ğŸ”§ Preparing core_weighted dataset...\n",
      "  ğŸ“Š Using 5 features: ['grid_pos', 'quali_pos', 'team_strength', 'driver_skill', 'gap_to_pole']\n",
      "  ğŸ“Š Shape after cleaning: X(240, 5), y(240,)\n",
      "  âš–ï¸ Applied weights (range: 0.75 - 0.95)\n",
      "  âœ… core_weighted dataset ready\n",
      "\n",
      "ğŸ”§ Preparing vae_optimized dataset...\n",
      "  ğŸ“Š Using 11 features: ['grid_pos', 'quali_pos', 'team_strength', 'driver_skill', 'gap_to_pole']...\n",
      "  ğŸ“Š Shape after cleaning: X(240, 11), y(240,)\n",
      "  âš–ï¸ Applied weights (range: 0.45 - 0.95)\n",
      "  âœ… vae_optimized dataset ready\n",
      "\n",
      "ğŸ”§ Preparing extended_weighted dataset...\n",
      "  ğŸ“Š Using 16 features: ['grid_pos', 'quali_pos', 'team_strength', 'driver_skill', 'gap_to_pole']...\n",
      "  ğŸ“Š Shape after cleaning: X(240, 16), y(240,)\n",
      "  âš–ï¸ Applied weights (range: 0.15 - 0.95)\n",
      "  âœ… extended_weighted dataset ready\n",
      "\n",
      "ğŸ“ˆ Feature weighting verification (core_weighted):\n",
      "  grid_pos                  | Weight: 0.950 | Weighted std:  0.952\n",
      "  quali_pos                 | Weight: 0.900 | Weighted std:  0.902\n",
      "  team_strength             | Weight: 0.850 | Weighted std:  0.852\n",
      "  driver_skill              | Weight: 0.800 | Weighted std:  0.802\n",
      "  gap_to_pole               | Weight: 0.750 | Weighted std:  0.752\n",
      "\n",
      "âœ… Feature selection and weighting complete!\n",
      "ğŸ“¦ Created 3 weighted datasets: ['core_weighted', 'vae_optimized', 'extended_weighted']\n"
     ]
    }
   ],
   "source": [
    "if not df.empty:\n",
    "    print(\"ğŸ“ Feature selection and scaling with importance weighting...\")\n",
    "    \n",
    "    # Get numeric columns only (exclude string columns)\n",
    "    numeric_columns = df_features.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    target = 'finish_pos'\n",
    "    \n",
    "    # Remove target and any non-predictive columns\n",
    "    exclude_cols = [target, 'year', 'points']  # Points is result, not predictor\n",
    "    available_features = [col for col in numeric_columns if col not in exclude_cols]\n",
    "    \n",
    "    print(f\"ğŸ“Š Available numeric features: {len(available_features)}\")\n",
    "    print(f\"ğŸ¯ Target variable: {target}\")\n",
    "    \n",
    "    # Get weighted features from config\n",
    "    config_weighted = get_weighted_features(\"all\")\n",
    "    config_high = get_weighted_features(\"high\")\n",
    "    config_medium = get_weighted_features(\"medium\")\n",
    "    \n",
    "    # Map config feature names to actual column names\n",
    "    def map_config_to_columns(config_features, available_cols):\n",
    "        \"\"\"Map config feature names to actual DataFrame columns\"\"\"\n",
    "        mapped = {}\n",
    "        for config_name, weight in config_features.items():\n",
    "            # Direct match\n",
    "            if config_name in available_cols:\n",
    "                mapped[config_name] = weight\n",
    "            # Pattern matching for similar names\n",
    "            else:\n",
    "                for col in available_cols:\n",
    "                    if config_name.replace('_', '') in col.replace('_', '') or col.replace('_', '') in config_name.replace('_', ''):\n",
    "                        mapped[col] = weight\n",
    "                        break\n",
    "        return mapped\n",
    "    \n",
    "    # Map config weights to available features\n",
    "    available_weighted = map_config_to_columns(config_weighted, available_features)\n",
    "    available_high = map_config_to_columns(config_high, available_features)\n",
    "    available_medium = map_config_to_columns(config_medium, available_features)\n",
    "    \n",
    "    # Add default weights for important features not in config\n",
    "    important_patterns = {\n",
    "        'driver_skill': 0.80,\n",
    "        'team_strength': 0.85,\n",
    "        'grid_pos': 0.95,\n",
    "        'quali_pos': 0.90,\n",
    "        'pit_stops': 0.60,\n",
    "        'gap_to_pole': 0.75,\n",
    "        'q3_time': 0.65,\n",
    "        'q2_time': 0.55,\n",
    "        'driver_experience': 0.50,\n",
    "        'year_normalized': 0.45\n",
    "    }\n",
    "    \n",
    "    for pattern, weight in important_patterns.items():\n",
    "        for col in available_features:\n",
    "            if pattern in col and col not in available_weighted:\n",
    "                if weight >= 0.8:\n",
    "                    available_high[col] = weight\n",
    "                elif weight >= 0.6:\n",
    "                    available_medium[col] = weight\n",
    "                available_weighted[col] = weight\n",
    "    \n",
    "    print(f\"\\nâš–ï¸ Feature importance summary:\")\n",
    "    print(f\"  â€¢ High importance: {len(available_high)} features (weight â‰¥ 0.8)\")\n",
    "    print(f\"  â€¢ Medium importance: {len(available_medium)} features (weight â‰¥ 0.6)\")\n",
    "    print(f\"  â€¢ All weighted: {len(available_weighted)} features\")\n",
    "    \n",
    "    # Show top weighted features\n",
    "    top_weighted = dict(sorted(available_weighted.items(), key=lambda x: x[1], reverse=True)[:10])\n",
    "    print(f\"\\nğŸ† Top weighted features:\")\n",
    "    for feature, weight in top_weighted.items():\n",
    "        print(f\"  â€¢ {feature:25}: {weight:.2f}\")\n",
    "    \n",
    "    # Create feature sets\n",
    "    core_features = list(available_high.keys())[:8]  # Top 8 high importance\n",
    "    vae_features = list(available_high.keys()) + list(available_medium.keys())\n",
    "    vae_features = list(dict.fromkeys(vae_features))[:15]  # Remove duplicates, limit to 15\n",
    "    extended_features = list(available_weighted.keys())[:20]  # Top 20 weighted features\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Feature set composition:\")\n",
    "    print(f\"  â€¢ Core: {len(core_features)} features\")\n",
    "    print(f\"  â€¢ VAE optimized: {len(vae_features)} features\") \n",
    "    print(f\"  â€¢ Extended: {len(extended_features)} features\")\n",
    "    \n",
    "    # Create feature sets with weights\n",
    "    feature_sets = {\n",
    "        'core_weighted': {\n",
    "            'features': core_features,\n",
    "            'weights': {f: available_weighted.get(f, 0.5) for f in core_features}\n",
    "        },\n",
    "        'vae_optimized': {\n",
    "            'features': vae_features,\n",
    "            'weights': {f: available_weighted.get(f, 0.5) for f in vae_features}\n",
    "        },\n",
    "        'extended_weighted': {\n",
    "            'features': extended_features,\n",
    "            'weights': {f: available_weighted.get(f, 0.3) for f in extended_features}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Prepare weighted datasets\n",
    "    datasets = {}\n",
    "    scalers = {}\n",
    "    \n",
    "    for set_name, fset in feature_sets.items():\n",
    "        print(f\"\\nğŸ”§ Preparing {set_name} dataset...\")\n",
    "        \n",
    "        features = fset['features']\n",
    "        weights = fset['weights']\n",
    "        \n",
    "        # Skip if no features available\n",
    "        if len(features) == 0:\n",
    "            print(f\"  âš ï¸ No features available for {set_name}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Ensure all features exist in dataframe\n",
    "        existing_features = [f for f in features if f in df_features.columns]\n",
    "        if len(existing_features) != len(features):\n",
    "            missing = set(features) - set(existing_features)\n",
    "            print(f\"  âš ï¸ Missing features: {missing}\")\n",
    "            features = existing_features\n",
    "            weights = {f: weights[f] for f in existing_features}\n",
    "        \n",
    "        print(f\"  ğŸ“Š Using {len(features)} features: {features[:5]}{'...' if len(features) > 5 else ''}\")\n",
    "        \n",
    "        # Extract features and target\n",
    "        X = df_features[features].copy()\n",
    "        y = df_features[target].copy()\n",
    "        \n",
    "        # Remove any rows with missing values\n",
    "        valid_mask = ~(X.isnull().any(axis=1) | y.isnull())\n",
    "        X = X[valid_mask]\n",
    "        y = y[valid_mask]\n",
    "        \n",
    "        print(f\"  ğŸ“Š Shape after cleaning: X{X.shape}, y{y.shape}\")\n",
    "        \n",
    "        if len(X) == 0:\n",
    "            print(f\"  âŒ No valid data for {set_name}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "        \n",
    "        # Apply feature weights by multiplying scaled features\n",
    "        feature_weights_array = np.array([weights.get(col, 0.5) for col in X_scaled.columns])\n",
    "        X_weighted = X_scaled * feature_weights_array\n",
    "        \n",
    "        print(f\"  âš–ï¸ Applied weights (range: {feature_weights_array.min():.2f} - {feature_weights_array.max():.2f})\")\n",
    "        \n",
    "        # Store dataset and scaler\n",
    "        datasets[set_name] = {\n",
    "            'X': X_weighted,  # Weighted and scaled\n",
    "            'X_scaled': X_scaled,  # Only scaled\n",
    "            'X_raw': X,  # Raw features\n",
    "            'y': y,\n",
    "            'features': features,\n",
    "            'weights': weights\n",
    "        }\n",
    "        scalers[set_name] = scaler\n",
    "        \n",
    "        print(f\"  âœ… {set_name} dataset ready\")\n",
    "    \n",
    "    # Display feature weighting verification for first available dataset\n",
    "    if datasets:\n",
    "        first_dataset_name = list(datasets.keys())[0]\n",
    "        first_dataset = datasets[first_dataset_name]\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ Feature weighting verification ({first_dataset_name}):\")\n",
    "        weights = first_dataset['weights']\n",
    "        X = first_dataset['X']\n",
    "        \n",
    "        for col in list(X.columns)[:5]:  # Show first 5 features\n",
    "            weight = weights[col]\n",
    "            weighted_std = X[col].std()\n",
    "            print(f\"  {col:25} | Weight: {weight:.3f} | Weighted std: {weighted_std:6.3f}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Feature selection and weighting complete!\")\n",
    "    print(f\"ğŸ“¦ Created {len(datasets)} weighted datasets: {list(datasets.keys())}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No data available for feature selection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434dda54",
   "metadata": {},
   "source": [
    "## ğŸ² Train/Validation Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af97115f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ² Creating train/validation splits...\n",
      "\n",
      "ğŸ”„ Splitting core_weighted dataset...\n",
      "  ğŸ“… Temporal split - Train: 60 | Val: 180\n",
      "    Train years: [np.int64(2022), np.int64(2023), np.int64(2024)] | Val years: [np.int64(2025)]\n",
      "  ğŸ¯ Stratified split - Train: 180 | Val: 60\n",
      "\n",
      "ğŸ”„ Splitting vae_optimized dataset...\n",
      "  ğŸ“… Temporal split - Train: 60 | Val: 180\n",
      "    Train years: [np.int64(2022), np.int64(2023), np.int64(2024)] | Val years: [np.int64(2025)]\n",
      "  ğŸ¯ Stratified split - Train: 180 | Val: 60\n",
      "\n",
      "ğŸ”„ Splitting extended_weighted dataset...\n",
      "  ğŸ“… Temporal split - Train: 60 | Val: 180\n",
      "    Train years: [np.int64(2022), np.int64(2023), np.int64(2024)] | Val years: [np.int64(2025)]\n",
      "  ğŸ¯ Stratified split - Train: 180 | Val: 60\n",
      "\n",
      "ğŸ“Š Split analysis for core_weighted dataset:\n",
      "  ğŸ“… Temporal: 60 train, 180 val\n",
      "  ğŸ¯ Stratified: 180 train, 60 val\n",
      "\n",
      "ğŸ“ˆ Target distribution (finish position):\n",
      "  Train: mean=10.5, std=5.7\n",
      "  Val:   mean=10.4, std=5.8\n",
      "\n",
      "âœ… All splits created successfully!\n",
      "ğŸ“¦ Split datasets: ['core_weighted', 'vae_optimized', 'extended_weighted']\n"
     ]
    }
   ],
   "source": [
    "if not df.empty:\n",
    "    print(\"ğŸ² Creating train/validation splits...\")\n",
    "    \n",
    "    splits = {}\n",
    "    \n",
    "    for set_name, dataset in datasets.items():\n",
    "        print(f\"\\nğŸ”„ Splitting {set_name} dataset...\")\n",
    "        \n",
    "        X = dataset['X']\n",
    "        y = dataset['y']\n",
    "        \n",
    "        # Strategy 1: Temporal split (by year)\n",
    "        years = df_features.loc[X.index, 'year']\n",
    "        unique_years = sorted(years.unique())\n",
    "        \n",
    "        if len(unique_years) >= 2:\n",
    "            # Use earliest year(s) for training, latest for validation\n",
    "            if len(unique_years) == 2:\n",
    "                train_years = [unique_years[0]]\n",
    "                val_years = [unique_years[1]]\n",
    "            else:\n",
    "                train_years = unique_years[:-1]\n",
    "                val_years = [unique_years[-1]]\n",
    "            \n",
    "            temporal_train_mask = years.isin(train_years)\n",
    "            temporal_val_mask = years.isin(val_years)\n",
    "            \n",
    "            X_train_temporal = X[temporal_train_mask]\n",
    "            X_val_temporal = X[temporal_val_mask]\n",
    "            y_train_temporal = y[temporal_train_mask]\n",
    "            y_val_temporal = y[temporal_val_mask]\n",
    "            \n",
    "            print(f\"  ğŸ“… Temporal split - Train: {X_train_temporal.shape[0]} | Val: {X_val_temporal.shape[0]}\")\n",
    "            print(f\"    Train years: {train_years} | Val years: {val_years}\")\n",
    "        else:\n",
    "            X_train_temporal = X_val_temporal = None\n",
    "            y_train_temporal = y_val_temporal = None\n",
    "            print(f\"  âš ï¸ Insufficient years for temporal split\")\n",
    "        \n",
    "        # Strategy 2: Stratified random split (maintaining position distribution)\n",
    "        # Bin finish positions for stratification\n",
    "        y_binned = pd.cut(y, bins=5, labels=['Top5', 'Mid-High', 'Middle', 'Mid-Low', 'Bottom'])\n",
    "        \n",
    "        try:\n",
    "            X_train_strat, X_val_strat, y_train_strat, y_val_strat = train_test_split(\n",
    "                X, y, test_size=0.25, stratify=y_binned, random_state=42\n",
    "            )\n",
    "            print(f\"  ğŸ¯ Stratified split - Train: {X_train_strat.shape[0]} | Val: {X_val_strat.shape[0]}\")\n",
    "        except ValueError:\n",
    "            # Fallback to regular random split if stratification fails\n",
    "            X_train_strat, X_val_strat, y_train_strat, y_val_strat = train_test_split(\n",
    "                X, y, test_size=0.25, random_state=42\n",
    "            )\n",
    "            print(f\"  ğŸ”€ Random split - Train: {X_train_strat.shape[0]} | Val: {X_val_strat.shape[0]}\")\n",
    "        \n",
    "        # Store splits\n",
    "        splits[set_name] = {\n",
    "            'temporal': {\n",
    "                'X_train': X_train_temporal,\n",
    "                'X_val': X_val_temporal,\n",
    "                'y_train': y_train_temporal,\n",
    "                'y_val': y_val_temporal\n",
    "            },\n",
    "            'stratified': {\n",
    "                'X_train': X_train_strat,\n",
    "                'X_val': X_val_strat,\n",
    "                'y_train': y_train_strat,\n",
    "                'y_val': y_val_strat\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Create simple visualization for core dataset\n",
    "    if datasets:\n",
    "        first_dataset_name = list(datasets.keys())[0]\n",
    "        first_splits = splits[first_dataset_name]\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Split analysis for {first_dataset_name} dataset:\")\n",
    "        \n",
    "        # Temporal split stats\n",
    "        if first_splits['temporal']['X_train'] is not None:\n",
    "            temporal_train_size = len(first_splits['temporal']['X_train'])\n",
    "            temporal_val_size = len(first_splits['temporal']['X_val'])\n",
    "            print(f\"  ğŸ“… Temporal: {temporal_train_size} train, {temporal_val_size} val\")\n",
    "        \n",
    "        # Stratified split stats  \n",
    "        strat_train_size = len(first_splits['stratified']['X_train'])\n",
    "        strat_val_size = len(first_splits['stratified']['X_val'])\n",
    "        print(f\"  ğŸ¯ Stratified: {strat_train_size} train, {strat_val_size} val\")\n",
    "        \n",
    "        # Show target distribution in stratified splits\n",
    "        y_train = first_splits['stratified']['y_train']\n",
    "        y_val = first_splits['stratified']['y_val']\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ Target distribution (finish position):\")\n",
    "        print(f\"  Train: mean={y_train.mean():.1f}, std={y_train.std():.1f}\")\n",
    "        print(f\"  Val:   mean={y_val.mean():.1f}, std={y_val.std():.1f}\")\n",
    "    \n",
    "    print(\"\\nâœ… All splits created successfully!\")\n",
    "    print(f\"ğŸ“¦ Split datasets: {list(splits.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b05ef25",
   "metadata": {},
   "source": [
    "## ğŸ“ Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c943a43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Encoding categorical features...\n",
      "  ğŸï¸ Encoded 30 drivers\n",
      "  ğŸ Encoded 13 teams\n",
      "  ğŸ“… Encoded 4 years\n",
      "  ğŸ¯ Created position bins: ['Podium_Contender', 'Points_Scorer', 'Midfield', 'Backmarker']\n",
      "\n",
      "ğŸ“Š Categorical distributions:\n",
      "\n",
      "DRIVER (30 categories):\n",
      "Alexander Albon     1\n",
      "Carlos Sainz        1\n",
      "Charles Leclerc     1\n",
      "Daniel Ricciardo    1\n",
      "Esteban Ocon        1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "TEAM (13 categories):\n",
      "Alfa Romeo      1\n",
      "AlphaTauri      1\n",
      "Alpine          1\n",
      "Aston Martin    1\n",
      "Ferrari         1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "YEAR (4 categories):\n",
      "2022    1\n",
      "2023    1\n",
      "2024    1\n",
      "2025    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "FINISH POSITION BINS:\n",
      "finish_pos_binned\n",
      "Points_Scorer       61\n",
      "Podium_Contender    60\n",
      "Midfield            60\n",
      "Backmarker          59\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if not df.empty:\n",
    "    print(\"ğŸ“ Encoding categorical features...\")\n",
    "    \n",
    "    # Create categorical encodings for Bayesian Networks\n",
    "    categorical_encodings = {}\n",
    "    \n",
    "    # Driver encoding (for driver effects)\n",
    "    if 'driver_name' in df_features.columns:\n",
    "        driver_encoder = LabelEncoder()\n",
    "        df_features['driver_encoded'] = driver_encoder.fit_transform(df_features['driver_name'])\n",
    "        categorical_encodings['driver'] = {\n",
    "            'encoder': driver_encoder,\n",
    "            'classes': driver_encoder.classes_,\n",
    "            'n_classes': len(driver_encoder.classes_)\n",
    "        }\n",
    "        print(f\"  ğŸï¸ Encoded {len(driver_encoder.classes_)} drivers\")\n",
    "    \n",
    "    # Team encoding\n",
    "    if 'team' in df_features.columns:\n",
    "        team_encoder = LabelEncoder()\n",
    "        df_features['team_encoded'] = team_encoder.fit_transform(df_features['team'])\n",
    "        categorical_encodings['team'] = {\n",
    "            'encoder': team_encoder,\n",
    "            'classes': team_encoder.classes_,\n",
    "            'n_classes': len(team_encoder.classes_)\n",
    "        }\n",
    "        print(f\"  ğŸ Encoded {len(team_encoder.classes_)} teams\")\n",
    "    \n",
    "    # Year encoding (for temporal effects)\n",
    "    year_encoder = LabelEncoder()\n",
    "    df_features['year_encoded'] = year_encoder.fit_transform(df_features['year'])\n",
    "    categorical_encodings['year'] = {\n",
    "        'encoder': year_encoder,\n",
    "        'classes': year_encoder.classes_,\n",
    "        'n_classes': len(year_encoder.classes_)\n",
    "    }\n",
    "    print(f\"  ğŸ“… Encoded {len(year_encoder.classes_)} years\")\n",
    "    \n",
    "    # Finish position binning (for Bayesian Network discrete variables)\n",
    "    position_bins = [1, 5, 10, 15, 20]  # Top 5, Mid-high (6-10), Mid-low (11-15), Bottom (16-20)\n",
    "    position_labels = ['Podium_Contender', 'Points_Scorer', 'Midfield', 'Backmarker']\n",
    "    \n",
    "    df_features['finish_pos_binned'] = pd.cut(\n",
    "        df_features['finish_pos'], \n",
    "        bins=position_bins, \n",
    "        labels=position_labels, \n",
    "        include_lowest=True\n",
    "    )\n",
    "    \n",
    "    # Encode binned positions\n",
    "    pos_bin_encoder = LabelEncoder()\n",
    "    df_features['finish_pos_binned_encoded'] = pos_bin_encoder.fit_transform(df_features['finish_pos_binned'])\n",
    "    categorical_encodings['finish_pos_binned'] = {\n",
    "        'encoder': pos_bin_encoder,\n",
    "        'classes': pos_bin_encoder.classes_,\n",
    "        'n_classes': len(pos_bin_encoder.classes_),\n",
    "        'bins': position_bins,\n",
    "        'labels': position_labels\n",
    "    }\n",
    "    \n",
    "    print(f\"  ğŸ¯ Created position bins: {position_labels}\")\n",
    "    \n",
    "    # Display categorical distribution\n",
    "    print(\"\\nğŸ“Š Categorical distributions:\")\n",
    "    for name, encoding in categorical_encodings.items():\n",
    "        if name != 'finish_pos_binned':  # Skip position bins for now\n",
    "            value_counts = pd.Series(encoding['encoder'].inverse_transform(range(encoding['n_classes']))).value_counts()\n",
    "            print(f\"\\n{name.upper()} ({encoding['n_classes']} categories):\")\n",
    "            print(value_counts.head())\n",
    "    \n",
    "    # Position bin distribution\n",
    "    print(\"\\nFINISH POSITION BINS:\")\n",
    "    print(df_features['finish_pos_binned'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f79e2b3",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e1b286d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Saving preprocessed data and artifacts...\n",
      "ğŸ“Š Full engineered dataset: data/preprocessed/singapore_engineered_20251005_180927.csv\n",
      "ğŸ”§ Preprocessing artifacts: data/preprocessed/preprocessing_artifacts_singapore_20251005_180927.pkl\n",
      "  ğŸ’¾ core_weighted dataset saved\n",
      "  ğŸ’¾ vae_optimized dataset saved\n",
      "  ğŸ’¾ extended_weighted dataset saved\n",
      "ğŸ“‹ Preprocessing summary: data/preprocessed/preprocessing_summary_singapore_20251005_180927.json\n",
      "\n",
      "ğŸ‰ PREPROCESSING COMPLETE!\n",
      "============================================================\n",
      "ğŸ Circuit: SINGAPORE GP\n",
      "ğŸ“Š Original data: (240, 20)\n",
      "ğŸ”§ Engineered data: (240, 49)\n",
      "â­ Features created: 29\n",
      "ğŸ¯ Feature sets: 3\n",
      "ğŸ“‚ Categorical encodings: 4\n",
      "\n",
      "ğŸ“¦ Dataset Summary:\n",
      "  â€¢ core_weighted  :  5 features, 240 samples\n",
      "  â€¢ vae_optimized  : 11 features, 240 samples\n",
      "  â€¢ extended_weighted: 16 features, 240 samples\n",
      "\n",
      "âœ… Ready for VAE and Bayesian Network implementation!\n",
      "\n",
      "ğŸš€ NEXT STEPS:\n",
      "1. ğŸ“– Load artifacts: pickle.load(preprocessing_artifacts.pkl)\n",
      "2. ğŸ§  VAE Training: Use 'vae_optimized' scaled datasets\n",
      "3. ğŸ•¸ï¸ Bayesian Network: Use categorical encodings + raw features\n",
      "4. ğŸ² Validation: Use both temporal and stratified splits\n",
      "5. ğŸ¯ Target: finish_pos (continuous) or finish_pos_binned (discrete)\n",
      "\n",
      "ğŸ“ Key Files Created:\n",
      "  ğŸ”§ Artifacts: data/preprocessed/preprocessing_artifacts_singapore_20251005_180927.pkl\n",
      "  ğŸ“Š Engineered Data: data/preprocessed/singapore_engineered_20251005_180927.csv\n",
      "  ğŸ“‹ Summary: data/preprocessed/preprocessing_summary_singapore_20251005_180927.json\n"
     ]
    }
   ],
   "source": [
    "if not df.empty and 'datasets' in locals() and datasets:\n",
    "    print(\"ğŸ’¾ Saving preprocessed data and artifacts...\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs('data/preprocessed', exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Determine circuit name for filename\n",
    "    circuit_name = \"singapore\"  # Default\n",
    "    if data_source == \"weighted_prediction\" and 'data_source' in df.columns:\n",
    "        sample_source = df['data_source'].iloc[0].lower()\n",
    "        if 'singapore' in sample_source:\n",
    "            circuit_name = \"singapore\"\n",
    "        elif 'monaco' in sample_source:\n",
    "            circuit_name = \"monaco\"\n",
    "        elif 'japan' in sample_source:\n",
    "            circuit_name = \"japan\"\n",
    "    \n",
    "    # 1. Save the complete engineered dataset\n",
    "    full_dataset_path = f\"data/preprocessed/{circuit_name}_engineered_{timestamp}.csv\"\n",
    "    df_features.to_csv(full_dataset_path, index=False)\n",
    "    print(f\"ğŸ“Š Full engineered dataset: {full_dataset_path}\")\n",
    "    \n",
    "    # 2. Save feature sets and splits\n",
    "    preprocessing_artifacts = {\n",
    "        'datasets': datasets,\n",
    "        'splits': splits if 'splits' in locals() else {},\n",
    "        'scalers': scalers,\n",
    "        'categorical_encodings': categorical_encodings if 'categorical_encodings' in locals() else {},\n",
    "        'feature_sets': feature_sets if 'feature_sets' in locals() else {},\n",
    "        'circuit_name': circuit_name,\n",
    "        'timestamp': timestamp,\n",
    "        'original_shape': df.shape,\n",
    "        'engineered_shape': df_features.shape\n",
    "    }\n",
    "    \n",
    "    artifacts_path = f\"data/preprocessed/preprocessing_artifacts_{circuit_name}_{timestamp}.pkl\"\n",
    "    with open(artifacts_path, 'wb') as f:\n",
    "        pickle.dump(preprocessing_artifacts, f)\n",
    "    print(f\"ğŸ”§ Preprocessing artifacts: {artifacts_path}\")\n",
    "    \n",
    "    # 3. Save individual datasets for easy loading\n",
    "    for set_name, dataset in datasets.items():\n",
    "        # Scaled features\n",
    "        dataset['X'].to_csv(f\"data/preprocessed/{circuit_name}_{set_name}_X_scaled_{timestamp}.csv\")\n",
    "        # Raw features\n",
    "        dataset['X_raw'].to_csv(f\"data/preprocessed/{circuit_name}_{set_name}_X_raw_{timestamp}.csv\")\n",
    "        # Target\n",
    "        dataset['y'].to_csv(f\"data/preprocessed/{circuit_name}_{set_name}_y_{timestamp}.csv\")\n",
    "        print(f\"  ğŸ’¾ {set_name} dataset saved\")\n",
    "    \n",
    "    # 4. Create preprocessing summary\n",
    "    summary = {\n",
    "        'circuit': circuit_name,\n",
    "        'timestamp': timestamp,\n",
    "        'original_shape': list(df.shape),\n",
    "        'engineered_shape': list(df_features.shape),\n",
    "        'features_created': len(df_features.columns) - len(df.columns),\n",
    "        'datasets_created': len(datasets),\n",
    "        'dataset_info': {\n",
    "            set_name: {\n",
    "                'feature_count': len(dataset['features']),\n",
    "                'sample_count': len(dataset['X']),\n",
    "                'feature_list': dataset['features'][:10]  # First 10 features\n",
    "            } for set_name, dataset in datasets.items()\n",
    "        },\n",
    "        'categorical_encodings': len(categorical_encodings) if 'categorical_encodings' in locals() else 0,\n",
    "        'data_quality': {\n",
    "            'missing_values_handled': True,\n",
    "            'features_scaled': True,\n",
    "            'weights_applied': True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    summary_path = f\"data/preprocessed/preprocessing_summary_{circuit_name}_{timestamp}.json\"\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(f\"ğŸ“‹ Preprocessing summary: {summary_path}\")\n",
    "    \n",
    "    # Display final summary\n",
    "    print(\"\\nğŸ‰ PREPROCESSING COMPLETE!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"ğŸ Circuit: {circuit_name.upper()} GP\")\n",
    "    print(f\"ğŸ“Š Original data: {df.shape}\")\n",
    "    print(f\"ğŸ”§ Engineered data: {df_features.shape}\")\n",
    "    print(f\"â­ Features created: {len(df_features.columns) - len(df.columns)}\")\n",
    "    print(f\"ğŸ¯ Feature sets: {len(datasets)}\")\n",
    "    print(f\"ğŸ“‚ Categorical encodings: {len(categorical_encodings) if 'categorical_encodings' in locals() else 0}\")\n",
    "    \n",
    "    # Dataset breakdown\n",
    "    print(f\"\\nğŸ“¦ Dataset Summary:\")\n",
    "    for set_name, dataset in datasets.items():\n",
    "        print(f\"  â€¢ {set_name:15}: {len(dataset['features']):2d} features, {len(dataset['X']):3d} samples\")\n",
    "    \n",
    "    print(f\"\\nâœ… Ready for VAE and Bayesian Network implementation!\")\n",
    "    \n",
    "    # Next steps guidance\n",
    "    print(\"\\nğŸš€ NEXT STEPS:\")\n",
    "    print(\"1. ğŸ“– Load artifacts: pickle.load(preprocessing_artifacts.pkl)\")\n",
    "    print(\"2. ğŸ§  VAE Training: Use 'vae_optimized' scaled datasets\")\n",
    "    print(\"3. ğŸ•¸ï¸ Bayesian Network: Use categorical encodings + raw features\")\n",
    "    print(\"4. ğŸ² Validation: Use both temporal and stratified splits\")\n",
    "    print(\"5. ğŸ¯ Target: finish_pos (continuous) or finish_pos_binned (discrete)\")\n",
    "    \n",
    "    # Show file paths for easy reference\n",
    "    print(f\"\\nğŸ“ Key Files Created:\")\n",
    "    print(f\"  ğŸ”§ Artifacts: {artifacts_path}\")\n",
    "    print(f\"  ğŸ“Š Engineered Data: {full_dataset_path}\")\n",
    "    print(f\"  ğŸ“‹ Summary: {summary_path}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Cannot save preprocessing artifacts - no datasets created\")\n",
    "    if df.empty:\n",
    "        print(\"   Reason: No input data loaded\")\n",
    "    elif 'datasets' not in locals():\n",
    "        print(\"   Reason: Feature processing failed\")\n",
    "    elif not datasets:\n",
    "        print(\"   Reason: No valid datasets created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d80c65",
   "metadata": {},
   "source": [
    "## ğŸ”— VAE â†’ Bayesian Network Integration Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bc847916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— Creating VAE â†’ Bayesian Network integration datasets...\n",
      "ğŸ“Š Using 'vae_optimized' dataset for VAE integration\n",
      "âœ… VAE Input Dataset: data/preprocessed/singapore_vae_input_20251005_180927.csv\n",
      "   Shape: (240, 12)\n",
      "   Features: ['grid_pos', 'quali_pos', 'team_strength', 'driver_skill', 'gap_to_pole', 'pit_stops', 'q3_time', 'q2_time', 'driver_experience', 'year_normalized']...\n",
      "âœ… BN Input Template: data/preprocessed/singapore_bn_input_template_20251005_180927.csv\n",
      "   Shape: (240, 8)\n",
      "   Features: ['latent_dim_0', 'latent_dim_1', 'latent_dim_2', 'latent_dim_3', 'driver_encoded', 'team_encoded', 'year_encoded', 'finish_pos_binned_encoded']\n",
      "\n",
      "ğŸš€ INTEGRATION WORKFLOW:\n",
      "1. ğŸ§  Train VAE on vae_input dataset (continuous features)\n",
      "2. ğŸ”„ Encode features â†’ latent vectors (4D)\n",
      "3. ğŸ•¸ï¸ Replace latent_dim_* in bn_input_template with VAE output\n",
      "4. ğŸ¯ Train Bayesian Network: latent + categorical â†’ finish_pos_binned\n",
      "5. ğŸ² Predict position probabilities using hybrid VAE-BN model\n",
      "\n",
      "ğŸ“‹ Integration summary: data/preprocessed/singapore_vae_bn_integration_20251005_180927.json\n",
      "ğŸ‰ VAE â†’ BN integration datasets ready!\n"
     ]
    }
   ],
   "source": [
    "# Create VAE â†’ Bayesian Network integration datasets\n",
    "if not df.empty and 'datasets' in locals() and datasets:\n",
    "    print(\"ğŸ”— Creating VAE â†’ Bayesian Network integration datasets...\")\n",
    "    \n",
    "    # Get the best available dataset for VAE\n",
    "    vae_dataset_name = 'vae_optimized'\n",
    "    if vae_dataset_name not in datasets:\n",
    "        vae_dataset_name = list(datasets.keys())[0]  # Use first available dataset\n",
    "    \n",
    "    print(f\"ğŸ“Š Using '{vae_dataset_name}' dataset for VAE integration\")\n",
    "    \n",
    "    # VAE Input Dataset (continuous features for encoding)\n",
    "    vae_input_df = datasets[vae_dataset_name]['X_scaled'].copy()  # Use scaled (not weighted) for VAE training\n",
    "    vae_input_df['target'] = datasets[vae_dataset_name]['y']\n",
    "    \n",
    "    # BN Input Dataset Template (will receive VAE latent vectors + categorical features)\n",
    "    if 'categorical_encodings' in locals() and categorical_encodings:\n",
    "        categorical_features = []\n",
    "        for enc_name, enc_info in categorical_encodings.items():\n",
    "            col_name = f\"{enc_name}_encoded\"\n",
    "            if col_name in df_features.columns:\n",
    "                categorical_features.append(col_name)\n",
    "        \n",
    "        if categorical_features:\n",
    "            bn_categorical_df = df_features[categorical_features].copy()\n",
    "        else:\n",
    "            # Create minimal categorical encoding\n",
    "            bn_categorical_df = pd.DataFrame({\n",
    "                'year_encoded': LabelEncoder().fit_transform(df_features['year']),\n",
    "                'driver_encoded': LabelEncoder().fit_transform(df_features['driver_name']),\n",
    "                'team_encoded': LabelEncoder().fit_transform(df_features['team'])\n",
    "            })\n",
    "            categorical_features = list(bn_categorical_df.columns)\n",
    "    else:\n",
    "        # Create minimal categorical encoding\n",
    "        bn_categorical_df = pd.DataFrame({\n",
    "            'year_encoded': LabelEncoder().fit_transform(df_features['year']),\n",
    "            'driver_encoded': LabelEncoder().fit_transform(df_features['driver_name']),\n",
    "            'team_encoded': LabelEncoder().fit_transform(df_features['team'])\n",
    "        })\n",
    "        categorical_features = list(bn_categorical_df.columns)\n",
    "    \n",
    "    # Create BN input structure (latent vector placeholders + categorical)\n",
    "    latent_dims = 4  # VAE latent dimensions\n",
    "    bn_input_df = pd.DataFrame()\n",
    "    \n",
    "    # Add latent dimension placeholders\n",
    "    for i in range(latent_dims):\n",
    "        bn_input_df[f'latent_dim_{i}'] = 0.0  # Will be filled by VAE encoder\n",
    "    \n",
    "    # Add categorical features\n",
    "    for cat_feature in categorical_features:\n",
    "        if cat_feature in bn_categorical_df.columns:\n",
    "            bn_input_df[cat_feature] = bn_categorical_df[cat_feature]\n",
    "    \n",
    "    # Add target (binned for Bayesian Network)\n",
    "    # Create position bins if not exists\n",
    "    if 'finish_pos_binned_encoded' not in df_features.columns:\n",
    "        position_bins = [1, 5, 10, 15, 20]\n",
    "        position_labels = ['Podium_Contender', 'Points_Scorer', 'Midfield', 'Backmarker']\n",
    "        \n",
    "        df_features['finish_pos_binned'] = pd.cut(\n",
    "            df_features['finish_pos'], \n",
    "            bins=position_bins, \n",
    "            labels=position_labels, \n",
    "            include_lowest=True\n",
    "        )\n",
    "        \n",
    "        pos_encoder = LabelEncoder()\n",
    "        df_features['finish_pos_binned_encoded'] = pos_encoder.fit_transform(df_features['finish_pos_binned'])\n",
    "    \n",
    "    bn_input_df['finish_pos_binned_encoded'] = df_features['finish_pos_binned_encoded']\n",
    "    \n",
    "    # Save integration datasets\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    circuit_name = circuit_name if 'circuit_name' in locals() else \"singapore\"\n",
    "    \n",
    "    vae_input_path = f\"data/preprocessed/{circuit_name}_vae_input_{timestamp}.csv\"\n",
    "    bn_input_path = f\"data/preprocessed/{circuit_name}_bn_input_template_{timestamp}.csv\"\n",
    "    \n",
    "    vae_input_df.to_csv(vae_input_path, index=False)\n",
    "    bn_input_df.to_csv(bn_input_path, index=False)\n",
    "    \n",
    "    print(f\"âœ… VAE Input Dataset: {vae_input_path}\")\n",
    "    print(f\"   Shape: {vae_input_df.shape}\")\n",
    "    print(f\"   Features: {list(vae_input_df.columns)[:10]}{'...' if len(vae_input_df.columns) > 10 else ''}\")\n",
    "    \n",
    "    print(f\"âœ… BN Input Template: {bn_input_path}\")\n",
    "    print(f\"   Shape: {bn_input_df.shape}\")\n",
    "    print(f\"   Features: {list(bn_input_df.columns)}\")\n",
    "    \n",
    "    print(\"\\nğŸš€ INTEGRATION WORKFLOW:\")\n",
    "    print(\"1. ğŸ§  Train VAE on vae_input dataset (continuous features)\")\n",
    "    print(\"2. ğŸ”„ Encode features â†’ latent vectors (4D)\")\n",
    "    print(\"3. ğŸ•¸ï¸ Replace latent_dim_* in bn_input_template with VAE output\")\n",
    "    print(\"4. ğŸ¯ Train Bayesian Network: latent + categorical â†’ finish_pos_binned\")\n",
    "    print(\"5. ğŸ² Predict position probabilities using hybrid VAE-BN model\")\n",
    "    \n",
    "    # Show integration summary\n",
    "    integration_summary = {\n",
    "        'circuit': circuit_name,\n",
    "        'timestamp': timestamp,\n",
    "        'vae_input': {\n",
    "            'path': vae_input_path,\n",
    "            'shape': list(vae_input_df.shape),\n",
    "            'features': list(vae_input_df.columns),\n",
    "            'source_dataset': vae_dataset_name\n",
    "        },\n",
    "        'bn_input_template': {\n",
    "            'path': bn_input_path, \n",
    "            'shape': list(bn_input_df.shape),\n",
    "            'features': list(bn_input_df.columns),\n",
    "            'categorical_features': categorical_features,\n",
    "            'latent_dimensions': latent_dims\n",
    "        },\n",
    "        'integration_workflow': {\n",
    "            'step1': 'Train VAE on continuous features',\n",
    "            'step2': 'Encode features to latent space',\n",
    "            'step3': 'Combine latent + categorical for BN',\n",
    "            'step4': 'Train BN for position prediction',\n",
    "            'final_target': 'finish_pos_binned (4 categories)'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    integration_summary_path = f\"data/preprocessed/{circuit_name}_vae_bn_integration_{timestamp}.json\"\n",
    "    with open(integration_summary_path, 'w') as f:\n",
    "        json.dump(integration_summary, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ Integration summary: {integration_summary_path}\")\n",
    "    print(\"ğŸ‰ VAE â†’ BN integration datasets ready!\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ Cannot create VAE-BN integration datasets - no processed datasets available\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
